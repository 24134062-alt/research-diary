# Category VII: Memory & Compute Optimization

> **Tá»•ng quan**: Memory vÃ  compute optimization táº­p trung vÃ o giáº£i quyáº¿t "memory wall" - bottleneck giá»¯a compute speed vÃ  memory bandwidth trong AI workloads.

---

## 66. Compute-in-Memory (CIM) for Neural Networks

### MÃ´ táº£
Thá»±c hiá»‡n computation trá»±c tiáº¿p trong memory arrays, trÃ¡nh data movement tá»‘n kÃ©m.

### The Memory Wall
```
Problem:
Compute speed improving faster than memory bandwidth
â†’ Memory becomes bottleneck

Traditional:
Memory â”€â”€loadâ”€â”€> Compute â”€â”€storeâ”€â”€> Memory
        â†‘____________â†“
        Lots of data movement!

CIM:
Memory + Compute integrated
        â†‘
        No data movement!
```

### CIM Approaches
| Technology | Principle | Precision |
|------------|-----------|-----------|
| **SRAM CIM** | Analog/digital in SRAM | 4-8 bit |
| **ReRAM CIM** | Resistance-based | 1-4 bit |
| **Flash CIM** | Charge-based | 4-8 bit |
| **DRAM CIM** | In-DRAM processing | Digital |

### Matrix-Vector Multiplication in ReRAM
```
Vâ‚ â”€â”€â”   â”Œâ”€ Râ‚â‚ â”€ Râ‚â‚‚ â”€ Râ‚â‚ƒ â”€â”
Vâ‚‚ â”€â”€â”¤   â”‚  Râ‚‚â‚ â”€ Râ‚‚â‚‚ â”€ Râ‚‚â‚ƒ â”€â”¤
Vâ‚ƒ â”€â”€â”˜   â””â”€ Râ‚ƒâ‚ â”€ Râ‚ƒâ‚‚ â”€ Râ‚ƒâ‚ƒ â”€â”˜
                   â”‚
                   â–¼
         I = V Ã— R (Ohm's law!)
         
Weights encoded as resistance values
Input as voltage
Output as current (sum)
```

### Challenges
- Analog noise
- Limited precision
- Write endurance
- Programming overhead

### Äá»c thÃªm
- ISAAC (2016)
- PRIME (2016)
- Compute-in-Memory Survey (2020)

---

## 67. Processing-in-Memory Architectures

### MÃ´ táº£
Äáº·t processing elements gáº§n hoáº·c trong memory Ä‘á»ƒ reduce data movement.

### PIM Levels
```
Near-memory computing:
Memory â† Short distance â†’ Processing

In-memory computing:
Memory + Processing = Same unit
```

### HBM-PIM
```
Traditional HBM:           HBM-PIM:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DRAM      â”‚           â”‚ DRAM + PIM  â”‚
â”‚   Dies      â”‚           â”‚   Logic     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Logic     â”‚           â”‚   Logic     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                         â”‚
    â–¼                         â–¼
Limited IO bandwidth     Compute at memory
```

### Samsung HBM-PIM
- Processing near HBM memory stacks
- 2.9x faster than GPU for specific workloads
- 62% less energy

### Äá»c thÃªm
- Samsung HBM-PIM (2021)
- UPMEM PIM-DRAM
- Processing-in-Memory Survey (2022)

---

## 68. Memory-efficient Training Algorithms

### MÃ´ táº£
Algorithms giáº£m memory footprint trong training, cho phÃ©p train larger models.

### Memory during Training
```
Memory = Weights + Gradients + Optimizer states + Activations

For AdamW:
Weights:     N params (FP16: 2N bytes)
Gradients:   N params (FP16: 2N bytes)  
Optimizer:   2N params (FP32: 8N bytes) - momentum + variance
Activations: O(batch Ã— layers Ã— hidden)

Total â‰ˆ 12-16 bytes per parameter + activations
```

### Techniques
| Technique | Memory Reduction | Tradeoff |
|-----------|-----------------|----------|
| Mixed precision | 2x | Minimal |
| Gradient checkpointing | 3-5x | 1.3x compute |
| ZeRO Stage 1-3 | 4-8x | Communication |
| LoRA | 10-100x | Fine-tuning only |

### Gradient Checkpointing
```
Forward: Compute all, save only checkpoints
Backward: Recompute from checkpoints

Trade: Compute â†‘ 33%, Memory â†“ âˆšn
```

### Äá»c thÃªm
- ZeRO (Microsoft, 2020)
- Gradient Checkpointing (2016)
- FSDP (PyTorch)

---

## 69. Activation Checkpointing Strategies

### MÃ´ táº£
Chiáº¿n lÆ°á»£c lÆ°u vÃ  tÃ¡i tÃ­nh toÃ¡n activations Ä‘á»ƒ giáº£m memory.

### Problem
```
Deep networks: Save all activations for backward pass
Memory âˆ depth Ã— batch Ã— hidden size

GPT-3 (175B): Activations can exceed 1TB!
```

### Checkpointing Strategies
| Strategy | Memory | Recompute |
|----------|--------|-----------|
| **None** | O(n) | 0 |
| **Full** | O(1) | O(nÂ²) |
| **Selective** | O(âˆšn) | O(n) |
| **Adaptive** | Budget-based | Varies |

### Implementation
```python
# Without checkpointing
def forward(x):
    x1 = layer1(x)    # Save
    x2 = layer2(x1)   # Save  
    x3 = layer3(x2)   # Save
    return x3

# With checkpointing
@torch.utils.checkpoint
def forward(x):
    x1 = layer1(x)    # Not saved
    x2 = layer2(x1)   # Not saved (recompute in backward)
    x3 = layer3(x2)
    return x3
```

### Äá»c thÃªm
- Sublinear Memory Cost (2016)
- Activation Compression (2021)

---

## 70. Memory Bandwidth Optimization

### MÃ´ táº£
Tá»‘i Æ°u sá»­ dá»¥ng memory bandwidth - often the true bottleneck.

### Bandwidth vs Compute
```
Arithmetic Intensity = FLOPs / Bytes moved

If low arithmetic intensity â†’ Memory-bound
If high arithmetic intensity â†’ Compute-bound

Goal: Increase arithmetic intensity via reuse
```

### Optimization Techniques
1. **Data layout optimization**: Align with memory access patterns
2. **Tiling**: Process data in cache-friendly blocks
3. **Prefetching**: Load data before needed
4. **Compression**: Reduce bytes transferred

### Memory Access Patterns
```
Good (Coalesced):         Bad (Strided):
Thread 0 â†’ addr 0         Thread 0 â†’ addr 0
Thread 1 â†’ addr 1         Thread 1 â†’ addr 128
Thread 2 â†’ addr 2         Thread 2 â†’ addr 256
...                       ...
One memory transaction    Many memory transactions
```

### Äá»c thÃªm
- Roofline Model (2009)
- Memory Optimization Survey (2021)

---

## 71. Cache-aware Neural Network Design

### MÃ´ táº£
Thiáº¿t káº¿ networks Ä‘á»ƒ maximize cache hit rate.

### Cache Hierarchy Impact
```
Access latency:
L1 cache:  ~4 cycles
L2 cache:  ~12 cycles
L3 cache:  ~40 cycles
DRAM:      ~200 cycles

50x difference between L1 and DRAM!
```

### Cache-friendly Design
| Design Choice | Cache Impact |
|---------------|--------------|
| Smaller layers | Fit in cache |
| Depthwise conv | Higher reuse |
| Channel-last layout | Better locality |
| Power-of-2 dims | Avoid conflicts |

### Tiling for Cache
```python
# Naive: Poor cache usage
for i in range(H):
    for j in range(W):
        compute(A[i,j], B[i,j])

# Tiled: Better cache usage
for ti in range(0, H, TILE):
    for tj in range(0, W, TILE):
        for i in range(ti, min(ti+TILE, H)):
            for j in range(tj, min(tj+TILE, W)):
                compute(A[i,j], B[i,j])
```

---

## 72. DRAM/SRAM Trade-offs for Edge AI

### MÃ´ táº£
CÃ¢n báº±ng giá»¯a SRAM (fast, expensive, limited) vÃ  DRAM (slow, cheap, larger).

### Comparison
| Aspect | SRAM | DRAM |
|--------|------|------|
| Speed | Fast (~1ns) | Slow (~50ns) |
| Energy | Low (10fJ/bit) | High (100fJ/bit) |
| Density | Low | High |
| Cost | High | Low |
| Refresh | Not needed | Needed |

### Strategy for Edge
```
Model weights: Store in Flash/DRAM (accessed less)
Activations: Buffer in SRAM (accessed frequently)
Hot weights: Cache in SRAM

Small models: All in SRAM
Large models: Layer-wise loading from DRAM
```

### Memory Planning
```python
def memory_plan(model, sram_size, dram_size):
    for layer in model.layers:
        if layer.activation_size < sram_size:
            # Keep activations in SRAM
            layer.activation_memory = SRAM
        else:
            # Tile and stream from DRAM
            layer.activation_memory = DRAM
```

---

## 73. Non-volatile Memory for Neural Network Storage

### MÃ´ táº£
Sá»­ dá»¥ng NVM (Flash, ReRAM, PCM) Ä‘á»ƒ store models vá»›i low power retention.

### NVM Technologies
| Type | Write | Read | Endurance | Use Case |
|------|-------|------|-----------|----------|
| **Flash** | Slow | Fast | 10âµ | Model storage |
| **ReRAM** | Fast | Fast | 10â¶-10â¹ | CIM |
| **PCM** | Medium | Fast | 10â¸ | General |
| **MRAM** | Fast | Fast | 10Â¹âµ | Cache |

### Benefits for Edge AI
- No power needed to retain model
- Instant-on capability
- Reduce DRAM power

### Challenges
- Write endurance
- Write energy
- Reliability

---

## 74. ReRAM-based Neural Network Accelerators

### MÃ´ táº£
Sá»­ dá»¥ng Resistive RAM Ä‘á»ƒ implement neural network weights vÃ  compute.

### ReRAM Basics
```
Resistive RAM cell:
High Resistance State (HRS) = "0"
Low Resistance State (LRS)  = "1"

Multiple levels possible â†’ store analog weights
```

### ReRAM Crossbar Array
```
       Vâ‚  Vâ‚‚  Vâ‚ƒ  (Input voltages)
        â”‚   â”‚   â”‚
    â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€ Iâ‚ (Output current)
        â”‚   â”‚   â”‚
    â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€ Iâ‚‚
        â”‚   â”‚   â”‚
    â”€â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€ Iâ‚ƒ
        
    â— = ReRAM cell (resistance = weight)
    
    I = V Ã— G (conductance)
    â†’ Parallel MAC operations!
```

### Advantages
- Massively parallel MAC
- In-memory computing
- Low power

### Äá»c thÃªm
- ISAAC (2016)
- PipeLayer (2017)
- PUMA (2019)

---

## 75. Memory Compression for Inference

### MÃ´ táº£
Compress data in memory Ä‘á»ƒ reduce storage vÃ  bandwidth.

### What to Compress
```
Weights: Static, high compression possible
Activations: Dynamic, need fast compression
Gradients: Training only, can be lossy
```

### Compression Techniques
| Technique | Compression | Overhead |
|-----------|-------------|----------|
| **Zero compression** | 2-5x | Very low |
| **Sparse encoding** | 2-10x | Low |
| **Entropy coding** | 1.5-3x | Medium |
| **Delta encoding** | 1.5-2x | Low |

### Zero-value Compression
```
Dense: [0, 0, 3, 0, 0, 2, 0, 1]
Compressed: values=[3, 2, 1], indices=[2, 5, 7]
Or run-length: [2, 3, 2, 2, 1, 1] (count, value pairs)
```

### Äá»c thÃªm
- Deep Compression (2016)
- SparCE (2018)
- Memory-side Compression (2019)

---

## ğŸ“š Memory Optimization Summary

### Key Metrics
```
Memory Efficiency = Useful data / Total data moved
Compute Efficiency = Achieved FLOPs / Peak FLOPs
Energy Efficiency = Operations / Joule
```

### Design Guidelines
1. Minimize data movement
2. Maximize data reuse
3. Match precision to task
4. Exploit sparsity
5. Use appropriate memory hierarchy
