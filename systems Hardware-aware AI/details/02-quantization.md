# Category II: Quantization

> **T·ªïng quan**: Quantization l√† k·ªπ thu·∫≠t gi·∫£m ƒë·ªô ch√≠nh x√°c s·ªë h·ªçc c·ªßa weights v√† activations t·ª´ floating-point (FP32) xu·ªëng lower precision (INT8, INT4, binary) ƒë·ªÉ tƒÉng t·ªëc inference v√† gi·∫£m memory.

---

## 16. Mixed-Precision Quantization with Hardware Constraints

### M√¥ t·∫£
S·ª≠ d·ª•ng c√°c bit-widths kh√°c nhau cho c√°c layers kh√°c nhau trong network, t·ªëi ∆∞u cho hardware c·ª• th·ªÉ.

### √ù t∆∞·ªüng
```
Layer 1: 8-bit  ‚Üê  Sensitive layer, c·∫ßn precision cao
Layer 2: 4-bit  ‚Üê  Less sensitive, c√≥ th·ªÉ compress
Layer 3: 8-bit  ‚Üê  Critical for accuracy
Layer 4: 2-bit  ‚Üê  Highly compressible
```

### Hardware Considerations
- **Supported precisions**: Kh√¥ng ph·∫£i hardware n√†o c≈©ng h·ªó tr·ª£ m·ªçi bit-width
- **Precision switching cost**: Overhead khi chuy·ªÉn ƒë·ªïi gi·ªØa precisions
- **Memory alignment**: Certain bit-widths align better in memory

### Optimization Problem
```
minimize    Latency(quantized_model)
subject to  Accuracy_drop ‚â§ threshold
            Bit-widths ‚àà {2, 4, 8, 16, 32}
```

### ƒê·ªçc th√™m
- HAQ: Hardware-Aware Automated Quantization (MIT, 2019)
- Mixed Precision Quantization of DNNs (2018)

---

## 17. Quantization-aware Training for Ultra-Low Precision (Binary, Ternary)

### M√¥ t·∫£
Train networks v·ªõi ultra-low precision (1-2 bits), simulate quantization effects during training.

### Precision Levels
| Bits | Values | Memory Reduction |
|------|--------|------------------|
| Binary (1-bit) | {-1, +1} | 32x |
| Ternary (2-bit) | {-1, 0, +1} | 16x |
| 4-bit | 16 levels | 8x |
| 8-bit | 256 levels | 4x |

### Training Challenges
```
Forward Pass:  Use quantized weights ‚Üí discrete
Backward Pass: Need gradients ‚Üí continuous

Solution: Straight-Through Estimator (STE)
          ‚àÇL/‚àÇw ‚âà ‚àÇL/‚àÇw_quantized
```

### Binary Networks
```python
# Binary weight
w_binary = sign(w_real)  # +1 or -1

# Binary convolution: very efficient
# XNOR + popcount instead of multiply-accumulate
output = popcount(XNOR(input_binary, weight_binary))
```

### ƒê·ªçc th√™m
- BinaryConnect (2015)
- XNOR-Net (2016)
- ReActNet (2020)

---

## 18. Hardware-aware Post-Training Quantization

### M√¥ t·∫£
Quantize model ƒë√£ train xong m√† kh√¥ng c·∫ßn retrain, v·ªõi awareness v·ªÅ target hardware.

### Process
```
Trained FP32 Model ‚Üí Calibration ‚Üí Quantization ‚Üí Optimized INT8 Model
                         ‚Üì
                   (Small dataset
                    to determine
                    scale/zero-point)
```

### Calibration Methods
| Method | Description | Accuracy |
|--------|-------------|----------|
| **Min-Max** | Use min/max values | Low |
| **Percentile** | Use percentile values | Medium |
| **MSE** | Minimize quantization error | High |
| **Cross-entropy** | Minimize output difference | Highest |

### Hardware-aware Aspects
- Choose quantization scheme supported by hardware
- Per-tensor vs per-channel quantization
- Symmetric vs asymmetric quantization

### ƒê·ªçc th√™m
- Post-training Quantization (Google, 2018)
- NVIDIA TensorRT Quantization

---

## 19. Dynamic Quantization for Adaptive Inference

### M√¥ t·∫£
ƒêi·ªÅu ch·ªânh quantization dynamically based on input data ho·∫∑c runtime conditions.

### Approaches
1. **Input-dependent**: Adjust precision based on input complexity
2. **Layer-adaptive**: Different precision for different inputs per layer
3. **Runtime-adaptive**: Change precision based on battery/thermal state

### Benefits
```
Easy Input ‚Üí Lower Precision ‚Üí Faster, Less Energy
Hard Input ‚Üí Higher Precision ‚Üí More Accuracy
```

### Implementation
```python
def dynamic_forward(x):
    complexity = estimate_complexity(x)
    if complexity < threshold:
        return quantized_forward(x, bits=4)
    else:
        return quantized_forward(x, bits=8)
```

### ƒê·ªçc th√™m
- Dynamic Network Quantization (2019)
- Input-adaptive Quantization (2020)

---

## 20. Quantization for Transformer Models on Edge Devices

### M√¥ t·∫£
√Åp d·ª•ng quantization cho Transformers (BERT, GPT, ViT) ƒë·ªÉ deploy tr√™n edge devices.

### Transformer Challenges
- **Attention mechanism**: Softmax requires higher precision
- **Layer Norm**: Sensitive to quantization
- **Large models**: BERT-base = 110M params

### Key Observations
```
Transformer Layers:
‚îú‚îÄ‚îÄ Self-Attention  ‚Üí Most sensitive, need 8-bit
‚îú‚îÄ‚îÄ FFN Linear      ‚Üí Less sensitive, can use 4-bit
‚îú‚îÄ‚îÄ LayerNorm       ‚Üí Keep FP32 or use approximation
‚îî‚îÄ‚îÄ Embeddings      ‚Üí Can be heavily quantized
```

### Special Techniques
- **Outlier-aware quantization**: Handle outliers in activations
- **I-BERT**: Integer-only BERT inference
- **Q8BERT**: 8-bit quantized BERT

### ƒê·ªçc th√™m
- Q-BERT (2019)
- I-BERT (2021)
- TernaryBERT (2020)

---

## 21. Layer-wise Optimal Bit-width Allocation

### M√¥ t·∫£
X√°c ƒë·ªãnh bit-width t·ªëi ∆∞u cho t·ª´ng layer ƒë·ªÉ maximize accuracy d∆∞·ªõi constraint v·ªÅ model size ho·∫∑c latency.

### Problem Formulation
```
maximize    Accuracy(model)
subject to  Œ£(bits_i √ó params_i) ‚â§ Budget
            bits_i ‚àà {2, 4, 8}
```

### Sensitivity Analysis
```
Sensitivity Score per Layer:
Layer 1: 0.85  ‚Üê High sensitivity, need more bits
Layer 2: 0.23  ‚Üê Low sensitivity, can compress
Layer 3: 0.67  ‚Üê Medium sensitivity
...
```

### Methods
1. **Heuristic**: Assign bits based on sensitivity ranking
2. **Optimization**: Use integer programming
3. **Learning-based**: Use RL or gradient-based methods

### ƒê·ªçc th√™m
- Mixed Precision DNNs (2018)
- HAWQ (2019)

---

## 22. Quantization Error Compensation Techniques

### M√¥ t·∫£
C√°c k·ªπ thu·∫≠t b√π ƒë·∫Øp cho accuracy loss do quantization.

### Error Sources
```
Quantization Error = Round(x √ó scale) / scale - x
                   = Rounding error + Clipping error
```

### Compensation Techniques

| Technique | Description |
|-----------|-------------|
| **Bias Correction** | Adjust biases to correct mean shift |
| **AdaRound** | Learned rounding instead of nearest |
| **BRECQ** | Block-wise reconstruction |
| **QDrop** | Randomly drop quantization during training |

### Bias Correction Example
```python
# Original output: E[Wx]
# Quantized output: E[Q(W)x] ‚â† E[Wx]
# Correction: bias_new = bias + E[Wx] - E[Q(W)x]
```

### ƒê·ªçc th√™m
- Data-Free Quantization (2019)
- AdaRound (2020)
- BRECQ (2021)

---

## 23. Integer-only Inference Optimization

### M√¥ t·∫£
Thi·∫øt k·∫ø inference pipeline ho√†n to√†n b·∫±ng integer arithmetic, kh√¥ng c·∫ßn floating-point.

### Why Integer-only?
- Faster: Integer ops 2-4x faster than FP32
- Lower power: Integer units consume less energy
- Simpler hardware: No FPU needed

### Challenges
```
Typical Neural Network:
Conv ‚Üí BatchNorm ‚Üí ReLU ‚Üí ... ‚Üí Softmax
  ‚îÇ        ‚îÇ                        ‚îÇ
  ‚îî‚îÄ‚îÄ Needs FP for normalization ‚îÄ‚îÄ‚îÄ‚îò

Solution: Fuse BN into Conv, approximate Softmax
```

### Integer Operations
```
Affine quantization: x_int = round(x * scale) + zero_point
Computation: y_int = x_int * w_int
Requantization: Adjust scale for next layer
```

### ƒê·ªçc th√™m
- Quantization and Training of DNNs (Google, 2018)
- TFLite Quantization

---

## 24. Quantization-friendly Neural Network Design

### M√¥ t·∫£
Thi·∫øt k·∫ø network architectures t·ª´ ƒë·∫ßu ƒë·ªÉ ch√∫ng d·ªÖ quantize h∆°n.

### Design Principles
1. **Avoid large dynamic range**: Easier to quantize
2. **Use ReLU over other activations**: Better clipping behavior
3. **Uniform layer widths**: Consistent quantization
4. **Avoid skip connections with different precisions**: Matching issues

### Quantization-friendly vs Unfriendly
```
Unfriendly:                    Friendly:
‚îú‚îÄ‚îÄ Swish activation           ‚îú‚îÄ‚îÄ ReLU activation
‚îú‚îÄ‚îÄ Squeeze-and-Excitation     ‚îú‚îÄ‚îÄ Simple residual
‚îú‚îÄ‚îÄ Deep narrow layers         ‚îú‚îÄ‚îÄ Balanced layers
‚îî‚îÄ‚îÄ Large softmax              ‚îî‚îÄ‚îÄ Temperature-scaled softmax
```

### ƒê·ªçc th√™m
- Searching for Quantization-friendly Architectures (2020)

---

## 25. On-chip Quantization Calibration

### M√¥ t·∫£
Perform calibration v√† fine-tuning c·ªßa quantization parameters directly tr√™n deployed device.

### Motivation
- Lab calibration data ‚â† deployment data distribution
- Device-specific characteristics affect optimal quantization
- Enable continuous adaptation

### On-chip Process
```
Deployed Device:
1. Collect small calibration buffer
2. Compute optimal scales/zero-points
3. Update quantization parameters
4. Continue inference with new params
```

### Challenges
- Limited compute for calibration
- Limited memory for storing calibration data
- Need to maintain service during calibration

### ƒê·ªçc th√™m
- On-chip Hardware-aware Quantization (2023)

---

## 26. Gradient Quantization for Distributed Training

### M√¥ t·∫£
Quantize gradients trong distributed training ƒë·ªÉ reduce communication overhead.

### Communication Bottleneck
```
Worker 1 ‚îÄ‚îÄ‚îê
Worker 2 ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚Üí Parameter Server ‚îÄ‚îÄ‚Üí Updated Weights
Worker 3 ‚îÄ‚îÄ‚îò         ‚îÇ
    ‚Üë                ‚îÇ
    ‚îî‚îÄ‚îÄ Gradients (large!) ‚îÄ‚îÄ‚îò
```

### Gradient Compression
| Method | Compression | Accuracy Impact |
|--------|-------------|-----------------|
| 1-bit SGD | 32x | Low (with error feedback) |
| TernGrad | ~16x | Low |
| Top-K | Variable | Low with large K |
| Random-K | Variable | Medium |

### Error Feedback
```python
# Accumulate quantization error
error_buffer += gradient - quantize(gradient)
# Apply error to next gradient
next_gradient += error_buffer
```

### ƒê·ªçc th√™m
- 1-bit SGD (Microsoft, 2014)
- Deep Gradient Compression (2018)

---

## 27. Activation Quantization vs Weight Quantization Trade-offs

### M√¥ t·∫£
Ph√¢n t√≠ch v√† t·ªëi ∆∞u trade-offs gi·ªØa quantizing weights vs activations.

### Comparison
| Aspect | Weight Quantization | Activation Quantization |
|--------|--------------------|-----------------------|
| Memory saving | Model size | Runtime memory |
| Computation | Weight-stationary speedup | Both inputs quantized |
| Sensitivity | Usually less sensitive | More sensitive |
| Range | Static | Dynamic (input-dependent) |

### Mixed Strategy
```
Conservative:  W8A8  (8-bit weights, 8-bit activations)
Moderate:      W4A8  (more weight compression)
Aggressive:    W4A4  (maximum compression)
Ultra:         W2A8  (extreme weight compression)
```

### Hardware Implications
- Weight quantization: Reduce model loading time
- Activation quantization: Reduce memory bandwidth during compute
- Joint: Maximum efficiency but careful tuning needed

### ƒê·ªçc th√™m
- Trained Ternary Quantization (2017)
- PACT: Parameterized Clipping Activation (2018)

---

## üìö Quantization Toolbox

### Frameworks
| Framework | Features |
|-----------|----------|
| **PyTorch Quantization** | Dynamic, static, QAT |
| **TensorFlow Lite** | Post-training, QAT |
| **NVIDIA TensorRT** | INT8 calibration |
| **ONNX Runtime** | Cross-platform quantization |

### Common Workflow
```
1. Train FP32 model
2. Analyze sensitivity
3. Choose quantization strategy
4. Calibrate or QAT
5. Validate accuracy
6. Deploy
```
