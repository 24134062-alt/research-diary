# Category IX: TinyML & Edge AI

> **Tá»•ng quan**: TinyML vÃ  Edge AI táº­p trung vÃ o viá»‡c deploy machine learning trÃªn cÃ¡c thiáº¿t bá»‹ cá»±c ká»³ háº¡n cháº¿ vá» tÃ i nguyÃªn nhÆ° microcontrollers.

---

## 86. TinyML Model Optimization for Microcontrollers

### MÃ´ táº£
Tá»‘i Æ°u ML models Ä‘á»ƒ cháº¡y trÃªn MCUs vá»›i KB memory vÃ  MHz clock speeds.

### MCU Constraints
```
Typical MCU (ARM Cortex-M4):
â”œâ”€â”€ Flash:  256KB - 2MB (model storage)
â”œâ”€â”€ SRAM:   64KB - 512KB (runtime memory)
â”œâ”€â”€ Clock:  80-200 MHz
â”œâ”€â”€ Power:  10-100 mW
â””â”€â”€ No FPU or limited FP support
```

### Optimization Techniques
| Technique | Memory Reduction | Compute Reduction |
|-----------|-----------------|-------------------|
| INT8 quantization | 4x | 2-4x |
| Pruning | 2-10x | 2-10x |
| Architecture search | 10-100x | 10-100x |
| Operator fusion | 1.2-2x | 1.2-2x |

### MCUNet Approach
```
1. Neural Architecture Search for MCUs
2. TinyNAS: Search in micro-scale space
3. TinyEngine: Optimized inference engine
4. Result: ImageNet on 256KB SRAM!
```

### Äá»c thÃªm
- MCUNet (MIT, 2020)
- TinyML Book (O'Reilly, 2022)
- TensorFlow Lite Micro

---

## 87. On-device Learning with Limited Resources

### MÃ´ táº£
Thá»±c hiá»‡n training hoáº·c fine-tuning trá»±c tiáº¿p trÃªn edge devices.

### Why On-device Learning?
```
Cloud training:              On-device learning:
â”œâ”€â”€ Privacy issues           â”œâ”€â”€ Data stays local
â”œâ”€â”€ Latency                  â”œâ”€â”€ Real-time adaptation
â”œâ”€â”€ Connectivity required    â”œâ”€â”€ Works offline
â””â”€â”€ Data transfer costs      â””â”€â”€ Personalization
```

### Memory Challenge
```
Inference memory: Weights + Activations
Training memory:  Weights + Activations + Gradients + Optimizer states
                  ~4-10x more memory than inference!
```

### Techniques for On-device Training
1. **Sparse updates**: Only update subset of weights
2. **Gradient checkpointing**: Trade compute for memory
3. **Low-rank adaptation**: Train small adapter layers
4. **Quantized training**: Train in low precision

### Äá»c thÃªm
- On-device Training Survey (2022)
- TinyTL (MIT, 2020)

---

## 88. Federated Learning on Edge Devices

### MÃ´ táº£
Distributed training across edge devices mÃ  khÃ´ng chia sáº» raw data.

### Federated Learning Process
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Device1 â”‚     â”‚ Device2 â”‚     â”‚ Device3 â”‚
â”‚ Local   â”‚     â”‚ Local   â”‚     â”‚ Local   â”‚
â”‚ Trainingâ”‚     â”‚ Trainingâ”‚     â”‚ Trainingâ”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚               â”‚               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Server   â”‚
              â”‚ Aggregate  â”‚
              â”‚   Models   â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Updated â”‚   â”‚ Updated â”‚   â”‚ Updated â”‚
â”‚ Device1 â”‚   â”‚ Device2 â”‚   â”‚ Device3 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Challenges
| Challenge | Solution |
|-----------|----------|
| Non-IID data | FedProx, SCAFFOLD |
| Communication | Gradient compression |
| Heterogeneity | Personalization |
| Privacy | Differential privacy |
| Stragglers | Asynchronous aggregation |

### Äá»c thÃªm
- FedAvg (Google, 2017)
- Federated Learning Survey (2021)

---

## 89. Privacy-preserving Edge AI

### MÃ´ táº£
Techniques Ä‘á»ƒ protect user privacy trong edge AI deployments.

### Privacy Threats
```
Threats:
â”œâ”€â”€ Data leakage (raw data exposed)
â”œâ”€â”€ Model inversion (reconstruct inputs)
â”œâ”€â”€ Membership inference (detect training data)
â””â”€â”€ Gradient leakage (in federated learning)
```

### Privacy Techniques
| Technique | Protection | Overhead |
|-----------|------------|----------|
| **On-device inference** | Data stays local | None |
| **Differential privacy** | Statistical guarantees | Accuracy loss |
| **Secure aggregation** | Protect gradients | Computation |
| **Homomorphic encryption** | Compute on encrypted | Very high |
| **TEE** | Hardware isolation | Moderate |

### Differential Privacy
```python
# Add calibrated noise to gradients
noisy_gradient = gradient + Laplace(0, sensitivity/epsilon)

# Provides (Îµ, Î´)-differential privacy
# Smaller Îµ = more privacy, less accuracy
```

### Äá»c thÃªm
- Differential Privacy for ML (2016)
- Private AI Survey (2021)

---

## 90. Real-time Object Detection on MCUs

### MÃ´ táº£
Deploy object detection models nhÆ° YOLO trÃªn microcontrollers.

### Object Detection Progression
```
YOLO:        ~7M params,  ~70 FPS on GPU
YOLOv5-nano: ~1.9M params
Tiny models: ~100K params, ~10 FPS on MCU
```

### MCU-friendly Architectures
| Model | Params | SRAM | Flash | Accuracy |
|-------|--------|------|-------|----------|
| MobileNetV2-SSD | 2M | 265KB | 2MB | 20% mAP |
| YOLO-Fastest | 230K | 120KB | 900KB | 13% mAP |
| MCUNet-Det | 500K | 256KB | 1MB | 25% mAP |

### Optimization Pipeline
```
Full YOLO â†’ Prune â†’ Quantize â†’ NAS â†’ Optimize ops
    â”‚         â”‚        â”‚        â”‚        â”‚
    â”‚         â”‚        â”‚        â”‚        â””â”€ Fused ops, INT8
    â”‚         â”‚        â”‚        â””â”€ Tiny architecture
    â”‚         â”‚        â””â”€ INT8/INT4
    â”‚         â””â”€ 80% sparsity
    â””â”€ 7M params
```

### Äá»c thÃªm
- YOLO-Fastest (2020)
- Person Detection on MCU (Google, 2019)

---

## 91. Voice Recognition for Ultra-low Power Devices

### MÃ´ táº£
Keyword spotting vÃ  voice recognition trÃªn always-on devices.

### Always-on Voice Detection
```
Power budget: <1mW (battery life months/years)
Latency: <200ms
Accuracy: >95% for keywords

Pipeline:
Audio â†’ MFCC features â†’ Tiny DNN â†’ Keyword detected?
                            â”‚
                    ~10-50KB model
```

### Model Architectures
| Model | Size | Power | Accuracy |
|-------|------|-------|----------|
| DS-CNN | 50KB | 500Î¼W | 95% |
| BC-ResNet | 20KB | 200Î¼W | 92% |
| TC-ResNet | 30KB | 300Î¼W | 94% |

### Feature Extraction
```
Raw audio â†’ MFCC features:
- Window: 25ms
- Hop: 10ms  
- Features: 13 MFCC coefficients
- Stacked: 1s context = 40 frames Ã— 13 = 520 features
```

### Äá»c thÃªm
- Hello Edge (ARM, 2018)
- Keyword Spotting (Google Speech Commands)

---

## 92. Sensor Fusion on Resource-constrained Hardware

### MÃ´ táº£
Combine multiple sensor inputs cho AI inference trÃªn limited hardware.

### Sensor Types for Edge AI
```
Common sensors:
â”œâ”€â”€ IMU (accelerometer, gyroscope): ~100Hz, 6-9 channels
â”œâ”€â”€ Microphone: 16kHz, 1-4 channels
â”œâ”€â”€ Camera: 30Hz, 640Ã—480
â”œâ”€â”€ Pressure/Temperature: 1Hz
â””â”€â”€ GPS: 1Hz
```

### Fusion Strategies
| Level | Description | Compute |
|-------|-------------|---------|
| **Early** | Concatenate raw features | High |
| **Late** | Fuse predictions | Low |
| **Intermediate** | Fuse embeddings | Medium |

### Early Fusion Example
```python
def fused_model(imu_data, audio_data):
    imu_features = imu_encoder(imu_data)      # 32 dims
    audio_features = audio_encoder(audio_data) # 64 dims
    combined = concat(imu_features, audio_features)  # 96 dims
    return classifier(combined)
```

### Äá»c thÃªm
- Multi-modal TinyML (2021)
- Sensor Fusion Survey (2020)

---

## 93. Predictive Maintenance with TinyML

### MÃ´ táº£
Deploy anomaly detection vÃ  failure prediction trÃªn industrial edge devices.

### Use Cases
```
Industrial TinyML:
â”œâ”€â”€ Vibration analysis (motor bearings)
â”œâ”€â”€ Acoustic monitoring (equipment sounds)
â”œâ”€â”€ Thermal monitoring
â”œâ”€â”€ Power quality analysis
â””â”€â”€ Visual inspection (simple cameras)
```

### Anomaly Detection Models
| Approach | Model Size | Accuracy | Interpretability |
|----------|------------|----------|------------------|
| Autoencoder | 10-50KB | High | Low |
| One-class SVM | 1-5KB | Medium | High |
| Isolation Forest | 5-20KB | Medium | Medium |
| Threshold-based | <1KB | Low | Very High |

### Implementation Example
```python
# Vibration anomaly detection
class VibrateAnomalyDetector:
    def __init__(self):
        self.autoencoder = TinyAutoencoder(input_dim=32)
        self.threshold = 0.1
    
    def is_anomaly(self, vibration_fft):
        reconstruction = self.autoencoder(vibration_fft)
        error = mse(vibration_fft, reconstruction)
        return error > self.threshold
```

### Äá»c thÃªm
- TinyML for Industrial IoT (2021)
- Edge Impulse Case Studies

---

## 94. Wearable AI System Design

### MÃ´ táº£
Design AI systems cho wearables nhÆ° smartwatches, fitness trackers, health monitors.

### Wearable Constraints
```
Constraints:
â”œâ”€â”€ Battery: 100-500mAh (1-7 day life)
â”œâ”€â”€ Size: <1cmÂ³ for electronics
â”œâ”€â”€ Weight: <50g total
â”œâ”€â”€ Heat: Must not be noticeable to user
â””â”€â”€ Comfort: Minimal sensor contact
```

### Wearable AI Applications
| Application | Sensors | Model Size | Power |
|-------------|---------|------------|-------|
| Step counting | Accelerometer | <1KB | <10Î¼W |
| Activity recognition | IMU | 10-50KB | <1mW |
| Heart rate estimation | PPG | 5-20KB | <500Î¼W |
| Atrial fibrillation | ECG | 20-100KB | <2mW |
| Sleep staging | Multiple | 50-200KB | <5mW |

### Power-Accuracy Trade-off
```
Duty cycling:
â”œâ”€â”€ Continuous: 100% accuracy, 100% power
â”œâ”€â”€ 10% duty: 90% accuracy, 15% power
â”œâ”€â”€ 1% duty: 70% accuracy, 5% power
â””â”€â”€ Event-triggered: Variable accuracy, minimal power
```

### Äá»c thÃªm
- Wearable AI Survey (2022)
- Apple Watch Health Features

---

## 95. Battery-aware Inference Scheduling

### MÃ´ táº£
Schedule AI inference tasks based on battery state vÃ  usage patterns.

### Scheduling Factors
```
Consider:
â”œâ”€â”€ Battery level (current charge)
â”œâ”€â”€ Charging state (plugged in?)
â”œâ”€â”€ Time to next charge (predicted)
â”œâ”€â”€ Task urgency (real-time vs deferrable)
â””â”€â”€ Model importance/accuracy needs
```

### Dynamic Scheduling
```python
def schedule_inference(battery_level, task_urgency, model_options):
    if battery_level > 0.7:
        return model_options['high_accuracy']
    elif battery_level > 0.3:
        if task_urgency == 'high':
            return model_options['high_accuracy']
        else:
            return model_options['efficient']
    else:  # Low battery
        if task_urgency == 'critical':
            return model_options['efficient']
        else:
            return defer_task()
```

### Energy Harvesting
```
Supplement battery with:
â”œâ”€â”€ Solar cells
â”œâ”€â”€ Vibration harvesting
â”œâ”€â”€ RF harvesting
â””â”€â”€ Thermal harvesting

AI can predict energy availability
and schedule accordingly
```

### Äá»c thÃªm
- Energy-aware ML (2020)
- Battery-aware Computing Survey

---

## ğŸ“š TinyML Ecosystem

### Frameworks
| Framework | Supported HW | Features |
|-----------|--------------|----------|
| TensorFlow Lite Micro | All MCUs | Quantization, interpreter |
| Edge Impulse | Nordic, ST, Arm | End-to-end platform |
| CMSIS-NN | Arm Cortex-M | Optimized kernels |
| STM32Cube.AI | STM32 | ST optimization |
| TinyEngine | Various | Memory efficient |

### Development Workflow
```
1. Train on PC (TensorFlow/PyTorch)
2. Optimize (quantize, prune)
3. Convert (TFLite, ONNX)
4. Deploy (generate C code)
5. Profile (latency, memory)
6. Iterate
```
