# Category VI: Hardware Accelerators

> **Tá»•ng quan**: Hardware Accelerators lÃ  cÃ¡c pháº§n cá»©ng chuyÃªn dá»¥ng Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ tÄƒng tá»‘c cÃ¡c workloads cá»§a neural networks, tá»« GPU, TPU Ä‘áº¿n FPGA vÃ  ASIC.

---

## 54. FPGA-based Neural Network Accelerator Design

### MÃ´ táº£
Thiáº¿t káº¿ accelerators trÃªn FPGA Ä‘á»ƒ cháº¡y neural network inference vá»›i customizable architecture.

### FPGA Advantages
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FPGA Benefits                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ“ Reconfigurable logic                      â”‚
â”‚ âœ“ Low latency (no OS overhead)              â”‚
â”‚ âœ“ Power efficient                            â”‚
â”‚ âœ“ Custom precision (arbitrary bit-widths)   â”‚
â”‚ âœ“ Parallel processing                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### FPGA Resources
| Resource | Purpose | Typical Use |
|----------|---------|-------------|
| **LUTs** | Logic operations | Control, routing |
| **DSPs** | Multiply-accumulate | Convolutions, FC |
| **BRAMs** | On-chip memory | Weights, activations |
| **I/O** | External communication | Data loading |

### Design Flow
```
Neural Network â†’ HLS/RTL Design â†’ Synthesis â†’ Place & Route â†’ Bitstream
                      â†“
              Optimization:
              - Loop unrolling
              - Pipelining
              - Data reuse
```

### Äá»c thÃªm
- FINN (Xilinx, 2017)
- VTA (TVM, 2018)
- Vitis AI (Xilinx/AMD)

---

## 55. ASIC Design for Deep Learning Inference

### MÃ´ táº£
Custom chip design tá»‘i Æ°u cho deep learning inference workloads.

### ASIC vs Other Platforms
| Aspect | ASIC | FPGA | GPU |
|--------|------|------|-----|
| Performance | Best | Good | Good |
| Power | Best | Good | High |
| Flexibility | None | High | Medium |
| Development cost | Very High | Medium | Low |
| Time to market | Long | Medium | Short |

### Famous AI ASICs
```
Google TPU:      Matrix multiply focused
Apple Neural Engine: Mobile AI
Qualcomm NPU:    Mobile AI
Tesla FSD:       Autonomous driving
Cerebras WSE:    Wafer-scale chip
```

### Design Considerations
1. **Dataflow architecture**: How data moves through chip
2. **Memory hierarchy**: On-chip SRAM, HBM
3. **Precision support**: INT8, INT4, FP16
4. **Scalability**: Multi-chip configurations

### Äá»c thÃªm
- In-Datacenter Performance of TPU (Google, 2017)
- Eyeriss (MIT, 2017)

---

## 56. Reconfigurable Computing for Adaptive AI

### MÃ´ táº£
Architectures cÃ³ thá»ƒ reconfigure Ä‘á»ƒ match different workloads vÃ  models.

### Reconfigurability Levels
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Reconfigurability Spectrum       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ASIC  â† Fixed                        â”‚
â”‚ CGRA  â† Coarse-grained              â”‚
â”‚ FPGA  â† Fine-grained                â”‚
â”‚ GPU   â† Programmable                â”‚
â”‚ CPU   â† General purpose â†’           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Coarse-Grained Reconfigurable Arrays (CGRA)
- Processing elements vá»›i configurable interconnects
- Faster reconfiguration than FPGA
- Higher efficiency than GPU for specific workloads

### Use Cases
- Multi-model deployment
- Dynamic precision switching
- Adaptive computation based on input

### Äá»c thÃªm
- CGRA Survey (2021)
- Plasticine (Stanford, 2017)

---

## 57. Dataflow Architectures for Neural Networks

### MÃ´ táº£
Thiáº¿t káº¿ data movement patterns tá»‘i Æ°u Ä‘á»ƒ maximize reuse vÃ  minimize memory access.

### Dataflow Types
| Dataflow | Reuses | Best For |
|----------|--------|----------|
| **Weight stationary** | Weights | Large models |
| **Output stationary** | Partial sums | Deep networks |
| **Input stationary** | Activations | Wide layers |
| **Row stationary** | All types | Balanced |

### Weight Stationary Example
```
Load weights once into PE array
Stream inputs through
Accumulate outputs

PE: Processing Element
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ PE  â”‚ PE  â”‚ PE  â”‚â—„â”€ Same weights loaded
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ PE  â”‚ PE  â”‚ PE  â”‚â—„â”€ Different inputs stream
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ PE  â”‚ PE  â”‚ PE  â”‚â—„â”€ Outputs accumulated
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```

### Energy Breakdown
```
Energy per operation:
DRAM access:     200Ã— MAC energy
SRAM access:     6Ã— MAC energy
Register access: 1Ã— MAC energy
MAC operation:   1Ã— (baseline)

â†’ Maximize reuse at lower memory levels!
```

### Äá»c thÃªm
- Eyeriss (MIT, 2017) - Row stationary
- TPU (Google) - Weight stationary
- NVDLA (NVIDIA) - Design framework

---

## 58. Memory Hierarchy Optimization for DNN Accelerators

### MÃ´ táº£
Thiáº¿t káº¿ vÃ  tá»‘i Æ°u memory hierarchy Ä‘á»ƒ reduce bottlenecks.

### Memory Hierarchy
```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚    DRAM     â”‚  Size: GB, BW: 100GB/s
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
            â”‚  Global     â”‚  Size: MBs, BW: 1TB/s
            â”‚  Buffer     â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
            â”‚  PE Local   â”‚  Size: KBs, BW: 10TB/s
            â”‚  Memory     â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
            â”‚  Registers  â”‚  Size: Bytes
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Optimization Techniques
1. **Tiling**: Break large tensors into tiles that fit
2. **Double buffering**: Overlap compute and load
3. **Prefetching**: Load data before needed
4. **Compression**: Reduce data movement

### Äá»c thÃªm
- Memory-Centric DNN Accelerators (2019)
- Timeloop (NVIDIA, 2019)

---

## 59. Systolic Array Design for Matrix Operations

### MÃ´ táº£
Thiáº¿t káº¿ systolic arrays - regular PE arrays vá»›i rhythmic data flow cho matrix operations.

### Systolic Array Concept
```
Data flows like "blood through heart" (systolic)

Input A â†’  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”
           â”‚ PE â”‚â†’ â”‚ PE â”‚â†’ â”‚ PE â”‚â†’ Output C
           â””â”€â”¬â”€â”€â”˜  â””â”€â”¬â”€â”€â”˜  â””â”€â”¬â”€â”€â”˜
Input B â†“   â”‚       â”‚       â”‚
           â”Œâ–¼â”€â”€â”€â”  â”Œâ–¼â”€â”€â”€â”  â”Œâ–¼â”€â”€â”€â”
           â”‚ PE â”‚â†’ â”‚ PE â”‚â†’ â”‚ PE â”‚â†’
           â””â”€â”¬â”€â”€â”˜  â””â”€â”¬â”€â”€â”˜  â””â”€â”¬â”€â”€â”˜
             â”‚       â”‚       â”‚
           â”Œâ–¼â”€â”€â”€â”  â”Œâ–¼â”€â”€â”€â”  â”Œâ–¼â”€â”€â”€â”
           â”‚ PE â”‚â†’ â”‚ PE â”‚â†’ â”‚ PE â”‚â†’
           â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜
```

### Each PE Operation
```python
# Per PE per cycle
c += a * b  # MAC operation
# Pass a to right, b down
```

### TPU Systolic Array
- Google TPU v1: 256Ã—256 systolic array
- 65,536 MACs per cycle
- INT8/INT16 precision

### Äá»c thÃªm
- TPU Architecture (Google)
- Systolic Array Primer

---

## 60. Multi-chip Module Design for Large Models

### MÃ´ táº£
Thiáº¿t káº¿ systems vá»›i multiple chips Ä‘á»ƒ handle models quÃ¡ lá»›n cho single chip.

### Why Multi-chip?
```
Large Models (GPT-3: 175B params):
- Single chip: ~1-10B params max
- Need: 20+ chips for full model

Also: Higher throughput via parallelism
```

### Parallelism Strategies
| Strategy | Description | Use Case |
|----------|-------------|----------|
| **Data parallel** | Same model, different data | Training |
| **Model parallel** | Split model across chips | Large models |
| **Pipeline parallel** | Different layers on chips | Deep models |
| **Tensor parallel** | Split tensors | Large layers |

### Interconnect Design
```
Chip-to-chip communication:
â”œâ”€â”€ NVLink (NVIDIA): 900 GB/s
â”œâ”€â”€ Infinity Fabric (AMD): 512 GB/s  
â”œâ”€â”€ TPU Interconnect (Google): 4.5 TB/s per chip
â””â”€â”€ Custom PCIe: 32 GB/s per link
```

### Äá»c thÃªm
- Megatron-LM (NVIDIA, 2019)
- GPipe (Google, 2019)

---

## 61. 3D-stacked Memory Integration for AI Accelerators

### MÃ´ táº£
TÃ­ch há»£p 3D-stacked memory (HBM) vá»›i compute Ä‘á»ƒ reduce memory bottleneck.

### HBM (High Bandwidth Memory)
```
Traditional:        3D Stacked (HBM):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CPU    â”‚        â”‚   Memory Dies   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â”‚   â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â” â”‚
     â”‚             â”‚   â”‚   â”‚   â”‚   â”‚ â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”        â”‚   â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜ â”‚
â”‚  DRAM   â”‚        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚   Logic   â”‚ â”‚
Wide bus needed    â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”˜
                   Short vertical connections
```

### HBM Specifications
| Gen | Bandwidth | Capacity | Power |
|-----|-----------|----------|-------|
| HBM1 | 128 GB/s | 4GB | 1W/GB |
| HBM2 | 256 GB/s | 8GB | 0.8W/GB |
| HBM2e | 460 GB/s | 16GB | 0.7W/GB |
| HBM3 | 819 GB/s | 24GB | 0.6W/GB |

### Benefits for AI
- Reduce memory-bound bottlenecks
- Support larger batch sizes
- Enable bigger on-chip models

### Äá»c thÃªm
- HBM in AI Accelerators (2020)
- NVIDIA A100/H100 Architecture

---

## 62. Accelerator Virtualization for Multi-tenant Deployment

### MÃ´ táº£
Chia sáº» AI accelerators giá»¯a multiple users/models má»™t cÃ¡ch efficient.

### Virtualization Approaches
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Physical GPU                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ VM 1    â”‚ VM 2    â”‚ VM 3    â”‚ VM 4      â”‚
â”‚ Model A â”‚ Model B â”‚ Model A â”‚ Model C   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technologies
- **NVIDIA MIG**: Multi-Instance GPU
- **NVIDIA vGPU**: Virtual GPU
- **AMD MxGPU**: SR-IOV based
- **Intel GVT-g**: Graphics virtualization

### Challenges
- Resource isolation
- Performance interference
- Memory partitioning
- Scheduling fairness

### Äá»c thÃªm
- NVIDIA MIG Documentation
- GPU Virtualization Survey (2021)

---

## 63. Power Management for AI Accelerators

### MÃ´ táº£
Quáº£n lÃ½ power consumption Ä‘á»ƒ balance performance vÃ  energy efficiency.

### Power Components
```
Total Power = Dynamic + Static + I/O

Dynamic: Computation, data movement (âˆ activity)
Static:  Leakage (âˆ chip area, always on)
I/O:     External communication
```

### Power Management Techniques
| Technique | Description | Savings |
|-----------|-------------|---------|
| **DVFS** | Dynamic voltage/frequency | 20-50% |
| **Clock gating** | Stop unused blocks | 10-30% |
| **Power gating** | Cut power to blocks | 50-90% |
| **Precision switching** | Lower precision | 30-50% |

### TDP Management
```
Thermal Design Power (TDP):
â”œâ”€â”€ Sustained power limit
â”œâ”€â”€ Burst above TDP short-term
â”œâ”€â”€ Throttle when overheating
â””â”€â”€ Balance thermal vs performance
```

### Äá»c thÃªm
- Energy-efficient DNN Accelerators (2019)
- GPU Power Management

---

## 64. Thermal-aware Accelerator Design

### MÃ´ táº£
Thiáº¿t káº¿ accelerators vá»›i awareness vá» thermal constraints.

### Thermal Challenges
```
Heat generation â†’ Temperature rise â†’ Issues:
â”œâ”€â”€ Performance throttling
â”œâ”€â”€ Reliability degradation  
â”œâ”€â”€ Shorter lifespan
â””â”€â”€ Higher cooling costs
```

### Thermal Management
1. **Design-time**: Floorplanning, power distribution
2. **Run-time**: DVFS, task migration, workload scheduling
3. **Cooling**: Heatsinks, fans, liquid cooling

### Dark Silicon Problem
```
Not all transistors can be active simultaneously
due to power/thermal limits.

Solution: Heterogeneous design
- Some cores high-performance
- Some cores energy-efficient
- Dynamic switching based on thermal state
```

### Äá»c thÃªm
- Thermal-aware DNN Accelerators (2020)

---

## 65. Security Considerations in AI Hardware

### MÃ´ táº£
Báº£o máº­t hardware AI khá»i cÃ¡c attacks vÃ  data leakage.

### Threat Model
```
Threats:
â”œâ”€â”€ Model extraction (steal model)
â”œâ”€â”€ Input inference (reveal inputs)
â”œâ”€â”€ Side-channel attacks (timing, power)
â””â”€â”€ Adversarial attacks via hardware
```

### Security Measures
| Measure | Protects Against |
|---------|-----------------|
| **Encryption** | Data in transit/rest |
| **TEE** | Privileged access |
| **Memory encryption** | Physical access |
| **Constant-time ops** | Timing attacks |
| **Power noise** | Power analysis |

### Trusted Execution Environment (TEE)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Normal World               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Secure World (TEE)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   AI Model (Protected)       â”‚    â”‚
â”‚  â”‚   Input/Output (Encrypted)   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Äá»c thÃªm
- ML Hardware Security Survey (2021)
- NVIDIA Confidential Computing

---

## ğŸ“š Hardware Accelerator Ecosystem

### Major Players
| Company | Products | Focus |
|---------|----------|-------|
| NVIDIA | GPU, Tensor Cores | General AI |
| Google | TPU | Cloud AI |
| Apple | Neural Engine | Mobile AI |
| Intel | Habana, IPU | Datacenter |
| AMD | MI series | HPC/AI |
| Cerebras | WSE | Large models |
| Graphcore | IPU | Training |
