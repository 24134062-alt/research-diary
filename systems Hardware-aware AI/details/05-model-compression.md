# Category V: Model Compression

> **T·ªïng quan**: Model Compression bao g·ªìm c√°c k·ªπ thu·∫≠t ngo√†i pruning v√† quantization ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc v√† computation c·ªßa neural networks.

---

## 46. Low-rank Factorization for Efficient Inference

### M√¥ t·∫£
Ph√¢n t√≠ch ma tr·∫≠n weights th√†nh t√≠ch c·ªßa c√°c ma tr·∫≠n c√≥ rank th·∫•p h∆°n ƒë·ªÉ gi·∫£m computation.

### Concept
```
Original Weight Matrix W (m √ó n):
- Parameters: m √ó n
- Computation: O(mn)

Low-rank Factorization W ‚âà U √ó V:
- U: m √ó k
- V: k √ó n
- Parameters: k(m + n)  [if k << min(m,n)]
- Computation: O(k(m + n))
```

### Decomposition Methods
| Method | Description | Use Case |
|--------|-------------|----------|
| **SVD** | Singular Value Decomposition | General |
| **Tucker** | Multi-dimensional decomposition | Conv layers |
| **CP** | CANDECOMP/PARAFAC | 4D tensors |
| **TT** | Tensor Train | Very large layers |

### Example: Conv Layer Decomposition
```
Original: (C_out, C_in, K, K) ‚Üí C_out √ó C_in √ó K¬≤ params

CP decomposition:
(C_out, R) √ó (C_in, R) √ó (K, R) √ó (K, R)
Parameters: R √ó (C_out + C_in + 2K)
```

### ƒê·ªçc th√™m
- Speeding up CNNs with Low-rank Expansions (2014)
- Tucker Decomposition for CNNs (2016)

---

## 47. Weight Sharing Strategies for Hardware Efficiency

### M√¥ t·∫£
Multiple weights share same value, reducing unique parameters v√† enabling efficient codebook lookup.

### Concept
```
Original weights: [1.2, 0.8, 1.3, 0.9, 1.1, 0.7, 1.2, 0.8]
After clustering: [A,   B,   A,   B,   A,   B,   A,   B]
Codebook: A=1.2, B=0.8 (only 2 values to store)
```

### K-means Clustering
1. Cluster weights into K centroids
2. Replace each weight with nearest centroid
3. Store: index array + centroid codebook
4. Fine-tune centroids

### Compression Ratio
```
Original: 32 bits per weight
Clustered: log2(K) bits per index + K √ó 32 bits for codebook

Example with K=16, 1M weights:
Original: 32M bits
Clustered: 4M bits + 512 bits ‚âà 4M bits (8x compression)
```

### ƒê·ªçc th√™m
- Deep Compression (Han et al., 2016)
- Hashed Nets (2015)

---

## 48. Neural Network Compression for Real-time Applications

### M√¥ t·∫£
Compression techniques ƒë·∫∑c bi·ªát cho latency-critical applications.

### Real-time Requirements
| Application | Latency Budget | FPS Requirement |
|-------------|---------------|-----------------|
| Autonomous driving | <100ms | 10-30 FPS |
| Video conferencing | <50ms | 30 FPS |
| Gaming | <16ms | 60 FPS |
| VR/AR | <7ms | 90+ FPS |

### Compression for Latency
```
Focus areas:
‚îú‚îÄ‚îÄ Reduce memory bottlenecks (activation compression)
‚îú‚îÄ‚îÄ Reduce computation (pruning, quantization)
‚îú‚îÄ‚îÄ Reduce model loading (smaller size)
‚îî‚îÄ‚îÄ Reduce I/O overhead (operator fusion)
```

### Latency Profiling
```python
# Profile each component
breakdown = {
    'conv_layers': 45%,
    'fc_layers': 30%,
    'memory_ops': 15%,
    'other': 10%
}
# Focus compression on largest contributors
```

### ƒê·ªçc th√™m
- Real-time Neural Networks (2020)
- Latency-aware Compression (2019)

---

## 49. Joint Compression: Pruning + Quantization + Distillation

### M√¥ t·∫£
K·∫øt h·ª£p nhi·ªÅu techniques compression ƒë·ªÉ ƒë·∫°t maximum efficiency.

### Compression Pipeline
```
Original Model
    ‚îÇ
    ‚ñº
Knowledge Distillation (smaller architecture)
    ‚îÇ
    ‚ñº
Pruning (remove unimportant weights)
    ‚îÇ
    ‚ñº
Quantization (reduce precision)
    ‚îÇ
    ‚ñº
Highly Compressed Model
```

### Joint Training
```python
Loss = TaskLoss + 
       Œ± * SparsityRegularization +  # Pruning
       Œ≤ * QuantizationLoss +         # Quantization  
       Œ≥ * DistillationLoss           # Distillation
```

### Multiplicative Compression
```
Distillation: 4x smaller architecture
Pruning: 5x fewer weights
Quantization: 4x smaller (INT8)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: 80x compression!
```

### ƒê·ªçc th√™m
- Deep Compression (2016)
- Joint Quantization and Pruning (2019)

---

## 50. Compression-aware Training from Scratch

### M√¥ t·∫£
Train networks t·ª´ ƒë·∫ßu v·ªõi awareness v·ªÅ compression s·∫Ω ƒë∆∞·ª£c applied.

### Approach
```
Traditional: Train ‚Üí Compress ‚Üí Fine-tune
Compression-aware: Train with compression simulation ‚Üí Deploy
```

### Training Modifications
1. **Simulated quantization**: Fake quantize during forward pass
2. **Soft pruning**: Learnable pruning masks
3. **Progressive regularization**: Gradually increase sparsity

### Benefits
- Better final accuracy
- No fine-tuning needed
- Model "learns to be compressed"

### ƒê·ªçc th√™m
- Training Quantized Networks from Scratch (2019)
- Gradual Pruning (2018)

---

## 51. Dynamic Model Compression based on Input Complexity

### M√¥ t·∫£
Adjust compression level dynamically based on input difficulty.

### Concept
```
Easy Input:  Use highly compressed model ‚Üí Fast
Hard Input:  Use less compressed model  ‚Üí Accurate

Complexity Estimator ‚Üí Route to appropriate model path
```

### Multi-exit Architecture
```
Input ‚Üí Block1 ‚Üí [Exit1: very compressed] 
              ‚Üì
        Block2 ‚Üí [Exit2: moderately compressed]
              ‚Üì
        Block3 ‚Üí [Exit3: full model]
```

### Complexity Estimation
- **Confidence-based**: Exit if confidence > threshold
- **Entropy-based**: Exit if entropy < threshold
- **Learned**: Train classifier for input difficulty

### ƒê·ªçc th√™m
- Adaptive Neural Networks (2017)
- SkipNet (2018)

---

## 52. Compressing Vision Transformers for Edge Devices

### M√¥ t·∫£
√Åp d·ª•ng compression cho Vision Transformers (ViT) ƒë·ªÉ deploy tr√™n edge.

### ViT Challenges
```
ViT-Base: 86M params, 17.5 GFLOPs
DeiT-Base: 86M params, 17.5 GFLOPs

Compare to:
MobileNetV3: 5M params, 0.2 GFLOPs
```

### Compression Strategies
| Strategy | Description | Reduction |
|----------|-------------|-----------|
| **Patch pruning** | Remove uninformative patches | 30-50% |
| **Head pruning** | Remove attention heads | 20-40% |
| **Token reduction** | Merge similar tokens | 30-60% |
| **Layer dropping** | Skip layers | 20-40% |
| **Quantization** | INT8 attention | 4x |

### Efficient ViT Designs
- **DeiT**: Distillation-trained ViT
- **MobileViT**: Mobile-friendly ViT
- **EfficientViT**: Hardware-efficient design

### ƒê·ªçc th√™m
- DeiT (2021)
- MobileViT (Apple, 2021)
- EfficientViT (2023)

---

## 53. LLM Compression for On-device Inference

### M√¥ t·∫£
Compress Large Language Models (GPT, LLaMA) ƒë·ªÉ ch·∫°y tr√™n devices.

### LLM Sizes
| Model | Parameters | FP16 Size |
|-------|------------|-----------|
| GPT-2 | 1.5B | 3GB |
| LLaMA-7B | 7B | 14GB |
| LLaMA-13B | 13B | 26GB |
| LLaMA-70B | 70B | 140GB |

### Compression Techniques for LLMs
```
1. Quantization: FP16 ‚Üí INT4 (4x reduction)
2. Pruning: Remove 50% weights (2x)
3. Distillation: 7B ‚Üí 1B student (7x)
4. Combined: 50-200x reduction possible
```

### Key Papers
- **GPTQ**: Post-training quantization for GPT
- **QLoRA**: Quantized Low-Rank Adaptation
- **AWQ**: Activation-aware Weight Quantization
- **SqueezeLLM**: Sensitivity-based quantization

### On-device LLMs
```
Target: Run 7B model on smartphone
Memory: 4-8GB available
Approach: 4-bit quantization ‚Üí 3.5GB
Add: KV cache optimization, speculative decoding
```

### ƒê·ªçc th√™m
- GPTQ (2023)
- llama.cpp (open source)
- MLC-LLM (2023)

---

## üìö Compression Summary

### Technique Comparison
| Technique | Model Size | Latency | Accuracy | Complexity |
|-----------|------------|---------|----------|------------|
| Quantization | 4x | 2x faster | -1-2% | Low |
| Pruning | 2-10x | 1.5-3x | -1-5% | Medium |
| Distillation | 4-10x | 4-10x | -2-5% | High |
| Low-rank | 2-4x | 1.5-2x | -1-3% | Medium |
| Combined | 20-100x | 10-50x | -3-10% | High |
