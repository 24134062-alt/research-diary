# Category III: Pruning & Sparsity

> **Tá»•ng quan**: Pruning lÃ  ká»¹ thuáº­t loáº¡i bá» cÃ¡c connections hoáº·c neurons Ã­t quan trá»ng trong neural network Ä‘á»ƒ giáº£m computation vÃ  memory requirements.

---

## 28. Structured Pruning for Hardware Acceleration

### MÃ´ táº£
Pruning theo cáº¥u trÃºc (channels, filters, layers) thay vÃ¬ individual weights, dá»… táº­n dá»¥ng hardware acceleration.

### Unstructured vs Structured
```
Unstructured (Fine-grained):     Structured (Coarse-grained):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0 1 0 0 1 0  â”‚                 â”‚ 1 1 1 1 1 1  â”‚
â”‚ 1 0 0 1 0 0  â”‚  â†’  Sparse      â”‚ 0 0 0 0 0 0  â”‚  â†’  Remove entire
â”‚ 0 0 1 0 0 1  â”‚     matrix      â”‚ 1 1 1 1 1 1  â”‚     row/column
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Hard to accelerate               Easy to accelerate
```

### Types of Structured Pruning
| Type | Granularity | Speedup | Accuracy |
|------|-------------|---------|----------|
| Filter pruning | Remove entire filters | High | Lower |
| Channel pruning | Remove channels | High | Medium |
| Layer pruning | Remove layers | Very High | Lowest |
| Block pruning | Remove blocks | Medium | Higher |

### Hardware Benefits
- No sparse tensor operations needed
- Direct model size reduction
- Standard dense operations work

### Äá»c thÃªm
- Filter Pruning (2017)
- Channel Pruning for Accelerating CNNs (2017)

---

## 29. Dynamic Pruning during Inference

### MÃ´ táº£
Pruning decisions made dynamically based on input data at runtime.

### Concept
```
Easy Input â†’ Skip more computations â†’ Faster
Hard Input â†’ Use more network â†’ Accurate
```

### Approaches
1. **Early exit**: Exit at intermediate layer if confident
2. **Dynamic channel selection**: Choose channels based on input
3. **Adaptive depth**: Variable number of layers per input

### Early Exit Architecture
```
Input â†’ Block1 â†’ [Exit1?] â†’ Block2 â†’ [Exit2?] â†’ Block3 â†’ Output
            â”‚         â”‚
            â””â†’ Output if confident
```

### Gating Mechanism
```python
def dynamic_forward(x, block, gate):
    importance = gate(x)  # Learn which channels to use
    mask = importance > threshold
    return block(x * mask)
```

### Äá»c thÃªm
- Dynamic Neural Networks Survey (2021)
- SkipNet (2018)
- BlockDrop (2018)

---

## 30. Channel Pruning with Hardware Latency Constraints

### MÃ´ táº£
Prune channels Ä‘á»ƒ meet target latency constraints trÃªn specific hardware.

### Optimization
```
minimize    Î£ importance(channel_i) Ã— pruned_i
subject to  Latency(pruned_model) â‰¤ Target_Latency
            pruned_i âˆˆ {0, 1}
```

### Channel Importance Criteria
| Criterion | Description | Cost |
|-----------|-------------|------|
| L1-norm | Sum of absolute weights | Low |
| L2-norm | Sum of squared weights | Low |
| Gradient | Gradient-based importance | Medium |
| Fisher | Fisher information | High |
| Taylor | First-order Taylor expansion | Medium |

### Latency-aware Process
```
1. Profile latency of each channel on target hardware
2. Compute importance scores
3. Greedily prune least important channels
4. Stop when latency budget met
5. Fine-tune
```

### Äá»c thÃªm
- NetAdapt (Google, 2018)
- AMC: AutoML for Model Compression (2018)

---

## 31. N:M Sparsity Patterns for GPU/TPU Optimization

### MÃ´ táº£
Sparsity pattern trong Ä‘Ã³ chá»‰ N values trong má»—i M consecutive values lÃ  non-zero, Ä‘Æ°á»£c NVIDIA Ampere GPUs há»— trá»£ natively.

### 2:4 Sparsity Example
```
Original:    [1.2, 0.5, 0.8, 0.3, 0.9, 0.2, 0.7, 0.4]
2:4 Sparse:  [1.2, 0.0, 0.8, 0.0, 0.9, 0.0, 0.7, 0.0]
             Keep 2 largest in each group of 4
```

### Hardware Support
```
NVIDIA Ampere (A100):
- Sparse Tensor Cores
- 2:4 structured sparsity
- ~2x speedup with minimal accuracy loss
```

### Training for N:M Sparsity
1. Train dense network
2. Apply N:M mask
3. Fine-tune with mask fixed
4. Optional: repeat pruning + fine-tuning

### Äá»c thÃªm
- Accelerating Sparse Deep Neural Networks (NVIDIA, 2021)
- SR-STE: N:M Sparsity Training (2021)

---

## 32. Lottery Ticket Hypothesis for Efficient Networks

### MÃ´ táº£
Hypothesis: Má»i dense network chá»©a má»™t sparse subnetwork (winning ticket) cÃ³ thá»ƒ train Ä‘áº¿n same accuracy khi isolated.

### The Hypothesis
```
Dense Network (initialized) 
    â”‚
    â”œâ”€â”€ Contains "winning ticket"
    â”‚   (sparse subnetwork + initialization)
    â”‚
    â””â”€â”€ If found and trained from scratch,
        matches dense network performance
```

### Finding Winning Tickets
```python
# Iterative Magnitude Pruning (IMP)
for iteration in range(num_iterations):
    1. Train network to completion
    2. Prune p% smallest magnitude weights
    3. Reset remaining weights to initial values
    4. Repeat
```

### Key Insights
- Initialization matters (must keep original init)
- Winning tickets are transferable across datasets
- Leads to very sparse networks (1-10% remaining)

### Äá»c thÃªm
- The Lottery Ticket Hypothesis (Frankle & Carlin, 2019)
- Stabilizing the Lottery Ticket (2019)

---

## 33. Hardware-aware Filter Importance Scoring

### MÃ´ táº£
Thiáº¿t káº¿ importance metrics cho filter pruning cÃ³ tÃ­nh Ä‘áº¿n hardware characteristics.

### Traditional Importance Scores
```python
# L1-norm based
importance = torch.sum(torch.abs(filter_weights))

# Activation-based
importance = torch.mean(activations ** 2)

# Gradient-based
importance = torch.abs(weights * gradients)
```

### Hardware-aware Scoring
```python
importance_hw = (
    accuracy_importance * Î± +
    (1 / latency_contribution) * Î² +
    (1 / energy_contribution) * Î³
)
```

### Layer-specific Hardware Costs
| Layer Position | Latency Impact | Pruning Priority |
|----------------|----------------|------------------|
| Early layers | High (large activations) | Higher priority |
| Middle layers | Medium | Medium |
| Late layers | Low (small activations) | Lower priority |

### Äá»c thÃªm
- Hardware-aware Network Pruning (2020)
- Network Slimming (2017)

---

## 34. Pruning Large Language Models for Edge Deployment

### MÃ´ táº£
Ãp dá»¥ng pruning cho LLMs nhÆ° GPT, LLaMA Ä‘á»ƒ cháº¡y trÃªn edge devices.

### LLM Pruning Challenges
- Huge model sizes (7B - 175B parameters)
- Complex attention mechanisms
- Maintaining coherence sau pruning

### Structured Pruning for LLMs
```
LLM Structure:
â”œâ”€â”€ Embedding layers     â†’ Hard to prune
â”œâ”€â”€ Attention heads      â†’ Can prune some heads
â”œâ”€â”€ FFN layers           â†’ Can prune neurons
â””â”€â”€ Output layers        â†’ Keep intact
```

### Techniques
1. **Head pruning**: Remove entire attention heads
2. **Width pruning**: Reduce hidden dimensions
3. **Depth pruning**: Remove entire layers
4. **Vocabulary pruning**: Reduce embedding size

### Äá»c thÃªm
- SparseGPT (2023)
- LLM-Pruner (2023)
- Wanda (2023)

---

## 35. Sparse Tensor Core Utilization

### MÃ´ táº£
Tá»‘i Æ°u hÃ³a neural networks Ä‘á»ƒ táº­n dá»¥ng Sparse Tensor Cores trong modern GPUs.

### Tensor Core Evolution
```
Volta (V100):  Dense Tensor Cores only
Ampere (A100): Dense + Sparse Tensor Cores (2:4)
Hopper (H100): Enhanced sparse operations
```

### Requirements for Sparse TC
- 2:4 structured sparsity pattern
- Specific matrix dimensions
- Proper data layout

### Optimization Workflow
```
1. Train with sparsity-inducing regularization
2. Convert to 2:4 pattern
3. Fine-tune
4. Use cuSPARSELt library
5. Profile and optimize
```

### Äá»c thÃªm
- NVIDIA cuSPARSELt Documentation
- Structured Pruning for Tensor Cores (2021)

---

## 36. Co-design of Pruning Algorithms and Hardware

### MÃ´ táº£
Äá»“ng thiáº¿t káº¿ pruning algorithms vÃ  hardware architectures Ä‘á»ƒ maximize efficiency.

### Co-design Space
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Co-design Space               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Algorithm      â”‚    Hardware          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sparsity pattern â”‚ Sparse accelerator   â”‚
â”‚ Pruning ratio    â”‚ Memory organization  â”‚
â”‚ Granularity      â”‚ Datapath design      â”‚
â”‚ Regularity       â”‚ Index encoding       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example: Column-balanced Sparsity
- Algorithm: Ensure equal sparsity per column
- Hardware: Simplified load balancing logic

### Research Directions
- Hardware-algorithm matching
- Sparsity format optimization
- Custom pruning for custom accelerators

### Äá»c thÃªm
- EIE: Efficient Inference Engine (2016)
- Sparse Architecture Co-design (2020)

---

## 37. Progressive Pruning with Hardware Feedback

### MÃ´ táº£
Pruning iteratively vá»›i feedback tá»« actual hardware measurements.

### Iterative Process
```
Initial Model
    â”‚
    â”œâ”€â”€â†’ Prune small percentage
    â”‚        â”‚
    â”‚        â–¼
    â”‚    Deploy on hardware
    â”‚        â”‚
    â”‚        â–¼
    â”‚    Measure latency/accuracy
    â”‚        â”‚
    â”‚        â–¼
    â”‚    Adjust pruning strategy
    â”‚        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    Until target met
```

### Feedback Signals
- Actual latency (not estimated)
- Memory bandwidth utilization
- Energy consumption
- Thermal behavior

### Benefits
- More accurate than analytical models
- Captures complex hardware behaviors
- Adapts to hardware variations

### Äá»c thÃªm
- NetAdapt (2018)
- AMC (2018)

---

## ğŸ“š Pruning Toolbox

### Libraries
| Library | Features |
|---------|----------|
| **PyTorch Pruning** | Basic structured/unstructured |
| **NVIDIA NeMo** | LLM pruning |
| **Neural Magic** | Sparsity training |
| **Intel Neural Compressor** | Hardware-aware pruning |

### Best Practices
1. Start with sensitivity analysis
2. Use gradual pruning schedule
3. Fine-tune after pruning
4. Validate on target hardware
5. Consider sparsity + quantization together
