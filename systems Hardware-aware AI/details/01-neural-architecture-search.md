# Category I: Neural Architecture Search (NAS)

> **T·ªïng quan**: Neural Architecture Search l√† ph∆∞∆°ng ph√°p t·ª± ƒë·ªông h√≥a vi·ªác thi·∫øt k·∫ø ki·∫øn tr√∫c m·∫°ng neural thay v√¨ thi·∫øt k·∫ø th·ªß c√¥ng b·ªüi chuy√™n gia.

---

## 1. Hardware-aware Neural Architecture Search for Edge Devices

### M√¥ t·∫£
Thi·∫øt k·∫ø t·ª± ƒë·ªông c√°c ki·∫øn tr√∫c m·∫°ng neural ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho c√°c thi·∫øt b·ªã edge nh∆∞ smartphone, IoT sensors, microcontrollers.

### V·∫•n ƒë·ªÅ gi·∫£i quy·∫øt
- Thi·∫øt b·ªã edge c√≥ gi·ªõi h·∫°n v·ªÅ memory, compute, v√† nƒÉng l∆∞·ª£ng
- C·∫ßn models ƒë·∫°t accuracy cao nh∆∞ng v·∫´n ch·∫°y ƒë∆∞·ª£c tr√™n hardware h·∫°n ch·∫ø
- Trade-off gi·ªØa performance v√† deployability

### Ph∆∞∆°ng ph√°p ch√≠nh
```
Search Space ‚Üí Search Strategy ‚Üí Hardware Constraints ‚Üí Optimal Architecture
     ‚Üì              ‚Üì                    ‚Üì
  (Ops, Layers)   (RL, Evolution,    (Latency,
                   Gradient-based)    Energy, Memory)
```

### ƒê·ªçc th√™m
- MnasNet (Google, 2019)
- ProxylessNAS (MIT, 2019)
- FBNet (Facebook, 2019)

---

## 2. Differentiable NAS with Hardware Constraints

### M√¥ t·∫£
S·ª≠ d·ª•ng gradient descent ƒë·ªÉ t√¨m ki·∫øm ki·∫øn tr√∫c t·ªëi ∆∞u, t√≠ch h·ª£p c√°c r√†ng bu·ªôc ph·∫ßn c·ª©ng nh∆∞ differentiable loss terms.

### V·∫•n ƒë·ªÅ gi·∫£i quy·∫øt
- NAS truy·ªÅn th·ªëng r·∫•t t·ªën t√†i nguy√™n (h√†ng ngh√¨n GPU hours)
- C·∫ßn ph∆∞∆°ng ph√°p nhanh h∆°n ƒë·ªÉ t√¨m ki·∫øm
- Kh√≥ t·ªëi ∆∞u ƒë·ªìng th·ªùi accuracy v√† hardware metrics

### √ù t∆∞·ªüng c·ªët l√µi
```python
# Pseudo-code
Loss = CrossEntropy(output, target) + Œª * HardwareCost(architecture)
# HardwareCost c√≥ th·ªÉ l√† latency, energy, ho·∫∑c memory
# C·∫ßn HardwareCost ph·∫£i differentiable
```

### K·ªπ thu·∫≠t
- **Gumbel-Softmax**: L√†m cho discrete choices tr·ªü n√™n differentiable
- **Latency lookup tables**: Precompute latency cho t·ª´ng operation
- **Differentiable latency predictors**: Train neural network ƒë·ªÉ predict latency

### ƒê·ªçc th√™m
- DARTS (CMU, 2019)
- SNAS (SenseTime, 2019)
- FBNetV2 (Facebook, 2020)

---

## 3. Multi-Objective NAS: Balancing Accuracy, Latency, and Energy

### M√¥ t·∫£
T√¨m ki·∫øm ki·∫øn tr√∫c t·ªëi ∆∞u cho nhi·ªÅu m·ª•c ti√™u ƒë·ªìng th·ªùi, kh√¥ng ch·ªâ accuracy m√† c√≤n latency, energy consumption, memory footprint.

### V·∫•n ƒë·ªÅ gi·∫£i quy·∫øt
- Single-objective optimization kh√¥ng ƒë·ªß cho real-world deployment
- C·∫ßn Pareto-optimal solutions cho c√°c trade-offs kh√°c nhau
- User c√≥ th·ªÉ ch·ªçn model ph√π h·ª£p v·ªõi constraints c·ª• th·ªÉ

### Pareto Front Concept
```
Accuracy ‚Üë
    ‚îÇ      ‚óè  ‚óè  ‚óè ‚Üê Pareto Front (best trade-offs)
    ‚îÇ   ‚óè
    ‚îÇ ‚óè
    ‚îÇ‚óè
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Latency ‚Üë
```

### Ph∆∞∆°ng ph√°p
- **Weighted Sum**: Combine objectives v·ªõi weights
- **NSGA-II/III**: Evolutionary multi-objective optimization
- **Scalarization**: Transform multi-objective th√†nh single objective

### ƒê·ªçc th√™m
- LEMONADE (Bosch, 2019)
- NSGANetV2 (2020)
- DONNA (MIT, 2021)

---

## 4. Zero-shot NAS for Resource-Constrained Devices

### M√¥ t·∫£
ƒê√°nh gi√° v√† x·∫øp h·∫°ng c√°c ki·∫øn tr√∫c m√† kh√¥ng c·∫ßn training, s·ª≠ d·ª•ng c√°c proxy metrics c√≥ th·ªÉ compute nhanh.

### V·∫•n ƒë·ªÅ gi·∫£i quy·∫øt
- Training m·ªói architecture candidate r·∫•t t·ªën th·ªùi gian
- Resource-constrained scenarios kh√¥ng th·ªÉ afford nhi·ªÅu training runs
- C·∫ßn c√°ch nhanh ƒë·ªÉ filter out bad architectures

### Proxy Metrics ph·ªï bi·∫øn
| Metric | √ù nghƒ©a | Compute Cost |
|--------|---------|--------------|
| **#Parameters** | Model size | O(1) |
| **#MACs/FLOPs** | Computation | O(1) |
| **Gradient norm** | Trainability | O(1 forward-backward) |
| **Synflow** | Signal propagation | O(1 forward) |
| **NASWOT** | Architecture expressivity | O(mini-batch) |

### ƒê·ªçc th√™m
- Zero-Cost Proxies (2021)
- Training-free NAS (2021)
- ZenNAS (2021)

---

## 5. Transferable NAS across Heterogeneous Hardware Platforms

### M√¥ t·∫£
Thi·∫øt k·∫ø methods ƒë·ªÉ transfer NAS results t·ª´ m·ªôt hardware platform sang platform kh√°c m√† kh√¥ng c·∫ßn search l·∫°i t·ª´ ƒë·∫ßu.

### V·∫•n ƒë·ªÅ gi·∫£i quy·∫øt
- M·ªói hardware platform c√≥ characteristics kh√°c nhau
- Search ri√™ng cho t·ª´ng platform r·∫•t t·ªën k√©m
- C·∫ßn generalization across platforms

### Approaches
1. **Meta-learning**: Learn to adapt quickly to new hardware
2. **Hardware embedding**: Encode hardware characteristics as vectors
3. **Predictor adaptation**: Fine-tune latency predictors for new hardware

### ƒê·ªçc th√™m
- HAT (MIT, 2020)
- OFA (Once-for-All, MIT, 2020)
- APQ (2020)

---

## 6. Reinforcement Learning-based Hardware-aware NAS

### M√¥ t·∫£
S·ª≠ d·ª•ng RL agents ƒë·ªÉ explore search space v√† maximize reward function bao g·ªìm c·∫£ accuracy v√† hardware efficiency.

### Framework
```
Controller (RNN) ‚Üí Generate Architecture ‚Üí Train & Evaluate ‚Üí Reward
      ‚Üë                                                          ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      
Reward = Accuracy - Œª * (Latency / Target_Latency)
```

### Challenges
- **Sample inefficiency**: RL c·∫ßn nhi·ªÅu samples
- **Reward shaping**: C√°ch design reward function
- **Exploration-exploitation**: Balance gi·ªØa explore new architectures v√† exploit good ones

### Algorithms s·ª≠ d·ª•ng
- Policy Gradient (REINFORCE)
- Proximal Policy Optimization (PPO)
- Q-learning variants

### ƒê·ªçc th√™m
- NASNet (Google, 2018)
- MnasNet (Google, 2019)
- HAQ (MIT, 2019)

---

## 7. Evolutionary Algorithms for Hardware-efficient Architecture Search

### M√¥ t·∫£
S·ª≠ d·ª•ng evolutionary algorithms (EA) nh∆∞ genetic algorithms ƒë·ªÉ evolve neural network architectures v·ªõi hardware constraints.

### Process
```
Population ‚Üí Selection ‚Üí Crossover ‚Üí Mutation ‚Üí New Population
    ‚Üì            ‚Üì           ‚Üì           ‚Üì            ‚Üì
 [Arch1,     (Fittest    (Combine    (Random      [Arch1',
  Arch2,      survive)    parents)    changes)     Arch2',
  ...]                                             ...]
```

### Fitness Function
```python
fitness = accuracy * (target_latency / actual_latency) ^ Œ≤
# Œ≤ controls importance of latency constraint
```

### Advantages
- Naturally handles multi-objective optimization
- No gradient computation required
- Good for discrete, complex search spaces

### ƒê·ªçc th√™m
- AmoebaNet (Google, 2019)
- CARS (2020)
- Regularized Evolution (2019)

---

## 8. Once-for-All Networks: Train Once, Deploy Anywhere

### M√¥ t·∫£
Train m·ªôt "super-network" duy nh·∫•t c√≥ th·ªÉ extract ra nhi·ªÅu sub-networks ph√π h·ª£p v·ªõi c√°c hardware constraints kh√°c nhau.

### Concept
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Once-for-All Network          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ Contains all possible sub-nets  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  Mobile  ‚îÇ  Tablet  ‚îÇ   PC   ‚îÇ  Server  ‚îÇ  Cloud ‚îÇ
  ‚îÇ  subnet  ‚îÇ  subnet  ‚îÇ subnet ‚îÇ  subnet  ‚îÇ subnet ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Searchable Dimensions
- Depth (s·ªë layers)
- Width (s·ªë channels per layer)
- Kernel size
- Resolution (input size)

### Benefits
- Train once, reduce cost significantly
- Instant deployment to new hardware
- No separate search needed

### ƒê·ªçc th√™m
- Once-for-All (MIT Han Lab, 2020)
- BigNAS (Google, 2020)
- AttentiveNAS (Facebook, 2021)

---

## 9. Supernet Training for Hardware-aware Model Selection

### M√¥ t·∫£
Techniques ƒë·ªÉ train weight-sharing supernets hi·ªáu qu·∫£, enabling fair comparison gi·ªØa c√°c architectures.

### Weight Sharing Concept
```
Supernet: All architectures share weights
          ‚îÇ
          ‚îú‚îÄ‚îÄ Subnet A: Uses subset of weights
          ‚îú‚îÄ‚îÄ Subnet B: Uses different subset  
          ‚îî‚îÄ‚îÄ Subnet C: Another subset
```

### Challenges
- **Interference**: Different subnets competing for same weights
- **Fairness**: Ensuring all subnets get adequate training
- **Ranking consistency**: Supernet ranking ‚â† standalone ranking

### Solutions
- Progressive shrinking
- Sandwich rule training
- Knowledge distillation from full network

### ƒê·ªçc th√™m
- Single-Path NAS (2019)
- FairNAS (2019)
- SPOS (2020)

---

## 10. Latency Predictor Design for NAS

### M√¥ t·∫£
Thi·∫øt k·∫ø c√°c predictors c√≥ th·ªÉ estimate latency c·ªßa m·ªôt architecture tr√™n target hardware m√† kh√¥ng c·∫ßn actually run.

### Approaches
| Approach | Input | Accuracy | Speed |
|----------|-------|----------|-------|
| **Lookup Table** | Operation type | Medium | Very Fast |
| **Linear Model** | Op counts | Low | Very Fast |
| **MLP Predictor** | Architecture encoding | High | Fast |
| **GNN Predictor** | Computation graph | Very High | Medium |

### Latency Breakdown
```
Total Latency = Œ£(Op latency) + Memory transfer + Overhead

Op latency = f(op_type, input_size, hardware_config)
```

### ƒê·ªçc th√™m
- nn-Meter (Microsoft, 2021)
- BRP-NAS (2020)
- HELP (2021)

---

## 11. Memory-aware Neural Architecture Search

### M√¥ t·∫£
NAS v·ªõi focus ƒë·∫∑c bi·ªát v√†o memory constraints - peak memory usage, memory bandwidth, activation memory.

### Memory Components
```
Total Memory = Weight Memory + Activation Memory + Workspace
    ‚îÇ              ‚îÇ                ‚îÇ                ‚îÇ
    ‚îÇ         (Parameters)    (Intermediate)    (Temp buffers)
    ‚îÇ
    ‚îî‚îÄ‚îÄ Must fit in device RAM/SRAM
```

### Optimization Targets
- Peak memory reduction
- Memory access patterns
- Buffer reuse optimization

### Techniques
- In-place operations
- Activation checkpointing-aware search
- Memory-efficient operators

### ƒê·ªçc th√™m
- MemNAS (2020)
- MCUNet (MIT, 2020)

---

## 12. Energy-aware NAS for Battery-powered Devices

### M√¥ t·∫£
T·ªëi ∆∞u h√≥a energy consumption thay v√¨ ch·ªâ latency, quan tr·ªçng cho wearables, IoT, mobile devices.

### Energy Model
```
Energy = Dynamic Energy + Static Energy
       = Œ£(Op energy) + Leakage * Time

Op energy ‚àù #Memory accesses + #Computations
```

### Considerations
- Memory access energy >> Computation energy
- Battery capacity constraints
- Thermal throttling effects

### ƒê·ªçc th√™m
- EfficientNet (Google, 2019)
- GreenAI (2019)

---

## 13. NAS for Specialized Hardware Accelerators (TPU, NPU, FPGA)

### M√¥ t·∫£
Thi·∫øt k·∫ø NAS methods ƒë·∫∑c bi·ªát cho c√°c accelerators v·ªõi characteristics ƒë·ªôc ƒë√°o.

### Hardware Characteristics

| Accelerator | Strengths | NAS Considerations |
|-------------|-----------|-------------------|
| **TPU** | Matrix multiply, high throughput | Batch size, tensor shapes |
| **NPU** | Low power, fixed ops | Supported ops, precision |
| **FPGA** | Reconfigurable, customizable | Resource utilization, routing |

### FPGA-specific
- Look for parallelizable architectures
- Consider resource types (LUTs, DSPs, BRAMs)
- Pipeline-friendly designs

### ƒê·ªçc th√™m
- Co-Exploration (2019)
- FPGA-aware NAS (2020)

---

## 14. Automated Search Space Design for Hardware-aware NAS

### M√¥ t·∫£
T·ª± ƒë·ªông thi·∫øt k·∫ø search space thay v√¨ manually define, adapting to target hardware.

### Problem
- Manual search space design requires expertise
- Suboptimal search space leads to suboptimal results
- Different hardware may need different search spaces

### Approaches
1. **Search space shrinking**: Start large, prune irrelevant parts
2. **Search space growing**: Start small, expand promising regions
3. **Meta-learning**: Learn good search spaces from previous searches

### ƒê·ªçc th√™m
- Neural Predictor for NAS (2019)
- AutoSpace (2021)

---

## 15. Proxy Tasks for Efficient Hardware-aware NAS

### M√¥ t·∫£
S·ª≠ d·ª•ng smaller/simpler tasks l√†m proxy ƒë·ªÉ ƒë√°nh gi√° architectures nhanh h∆°n.

### Proxy Types
| Proxy | Description | Speedup |
|-------|-------------|---------|
| **Reduced epochs** | Train fewer epochs | 10-100x |
| **Reduced dataset** | Use subset of data | 10-50x |
| **Reduced resolution** | Lower input size | 2-5x |
| **Smaller model** | Scale down architecture | 5-20x |

### Correlation Challenge
```
Proxy performance ‚Üê?‚Üí Full training performance
                   ‚îÇ
        Need high rank correlation
```

### ƒê·ªçc th√™m
- NASBENCH (Google, 2019)
- Proxy validation studies (2020)

---

## üìö T√†i Li·ªáu T·ªïng H·ª£p

### Must-read Papers
1. "Neural Architecture Search: A Survey" (2019)
2. "A Survey on Hardware-aware Neural Architecture Search" (2021)
3. "Efficient Deep Learning: A Survey" (2020)

### Influential Works
- NASNet ‚Üí ƒë·∫∑t n·ªÅn t·∫£ng cho NAS
- DARTS ‚Üí differentiable approach
- Once-for-All ‚Üí practical deployment
- MnasNet ‚Üí hardware-aware pioneer

### Conferences
- NeurIPS, ICML, ICLR (top ML venues)
- CVPR, ICCV, ECCV (computer vision)
- DAC, ICCAD (hardware design)
- MLSys (systems for ML)
