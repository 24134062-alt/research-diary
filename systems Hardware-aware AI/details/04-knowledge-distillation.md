# Category IV: Knowledge Distillation

> **Tá»•ng quan**: Knowledge Distillation lÃ  ká»¹ thuáº­t chuyá»ƒn "kiáº¿n thá»©c" tá»« má»™t model lá»›n (teacher) sang model nhá» hÆ¡n (student) Ä‘á»ƒ student Ä‘áº¡t performance tÆ°Æ¡ng Ä‘Æ°Æ¡ng nhÆ°ng efficient hÆ¡n.

---

## 38. Hardware-aware Knowledge Distillation

### MÃ´ táº£
Thiáº¿t káº¿ student network vÃ  distillation process vá»›i awareness vá» target hardware constraints.

### Traditional vs Hardware-aware
```
Traditional:                     Hardware-aware:
Teacher â†’ Student                Teacher â†’ Student
         (fixed arch)                     (optimized for hardware)
                                          + Latency constraint
                                          + Memory constraint
                                          + Energy constraint
```

### Distillation vá»›i Hardware Constraints
```python
Loss = Î± * TaskLoss(student_output, labels) +
       Î² * DistillLoss(student_output, teacher_output) +
       Î³ * max(0, Latency(student) - TargetLatency)
```

### Joint Optimization
- Student architecture search + distillation
- Layer-wise distillation importance based on hardware
- Progressive distillation with hardware feedback

### Äá»c thÃªm
- Hardware-aware Knowledge Distillation (2021)
- Student-Teacher Networks for Edge AI (2020)

---

## 39. Self-distillation for Efficient Networks

### MÃ´ táº£
Model tá»± distill knowledge vÃ o chÃ­nh nÃ³, khÃ´ng cáº§n separate teacher.

### Concept
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Same Network               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Deep Path      â”‚    Shallow Path     â”‚
â”‚   (Teacher)      â”‚    (Student)        â”‚
â”‚        â”‚         â”‚         â–²           â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚          Distill â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Methods
1. **Be Your Own Teacher (BYOT)**: Later layers teach earlier layers
2. **Deep Mutual Learning**: Multiple networks teach each other
3. **Born-Again Networks**: Train new network with same architecture

### Benefits
- No need for separate teacher
- Single training process
- Regularization effect

### Äá»c thÃªm
- Self-Distillation (2019)
- Born-Again Neural Networks (2018)
- Deep Mutual Learning (2018)

---

## 40. Feature-based Distillation for Edge AI

### MÃ´ táº£
Distill intermediate feature representations, khÃ´ng chá»‰ final outputs.

### Feature Points for Distillation
```
Teacher Network:
Input â†’ [F1] â†’ [F2] â†’ [F3] â†’ [F4] â†’ Output
          â†“      â†“      â†“      â†“
Student Network:
Input â†’ [f1] â†’ [f2] â†’ [f3] â†’ [f4] â†’ Output

Distillation: Align F_i with f_i
```

### Alignment Methods
| Method | Operation | Cost |
|--------|-----------|------|
| **Direct matching** | MSE(F, f) | Low |
| **Attention transfer** | Match attention maps | Medium |
| **Gram matrices** | Match style/texture | Medium |
| **Contrastive** | Learn similarities | High |

### Adapter Layers
```python
# When dimensions don't match
class Adapter(nn.Module):
    def __init__(self, student_dim, teacher_dim):
        self.proj = nn.Linear(student_dim, teacher_dim)
    
    def forward(self, student_features):
        return self.proj(student_features)
```

### Äá»c thÃªm
- FitNets (2015)
- Attention Transfer (2017)
- Contrastive Representation Distillation (2020)

---

## 41. Distillation-aware Architecture Design

### MÃ´ táº£
Thiáº¿t káº¿ student architectures Ä‘áº·c biá»‡t tá»‘i Æ°u cho distillation.

### Design Considerations
```
Good Student for Distillation:
â”œâ”€â”€ Similar structure to teacher (easier mapping)
â”œâ”€â”€ Sufficient capacity to absorb knowledge
â”œâ”€â”€ Efficient computation
â””â”€â”€ Hardware-friendly operations
```

### Architecture Matching
| Teacher Component | Student Equivalent |
|------------------|-------------------|
| Large conv | Depthwise-separable conv |
| Attention | Linear attention / Local attention |
| Deep layers | Skip connections |
| Wide layers | Narrow + deeper |

### Capacity Analysis
```
Need: Student Capacity >= Essential Knowledge
But:  Student Capacity << Teacher Capacity

Sweet spot: Just enough capacity for task knowledge
```

### Äá»c thÃªm
- Structured Knowledge Distillation (2019)
- Architecture-aware KD (2020)

---

## 42. Multi-teacher Distillation for Robust Edge Models

### MÃ´ táº£
Sá»­ dá»¥ng nhiá»u teacher models Ä‘á»ƒ distill knowledge Ä‘a dáº¡ng vÃ o má»™t student.

### Multi-teacher Setup
```
Teacher 1 (ImageNet expert)  â”€â”
Teacher 2 (Detection expert)  â”œâ”€â†’ Student
Teacher 3 (Segmentation)     â”€â”˜
```

### Knowledge Aggregation
1. **Averaging**: Average teacher outputs
2. **Weighted**: Learnable weights per teacher
3. **Selective**: Choose best teacher per sample
4. **Ensemble**: Combine with attention

### Benefits
- More robust knowledge
- Better generalization
- Task-specific expertise

### Äá»c thÃªm
- Multi-teacher Knowledge Distillation (2020)
- Ensemble Knowledge Distillation (2019)

---

## 43. Online Distillation on Resource-constrained Devices

### MÃ´ táº£
Perform distillation trá»±c tiáº¿p trÃªn edge devices vá»›i limited resources.

### Challenges
```
Edge Device Constraints:
â”œâ”€â”€ Limited memory: Can't load large teacher
â”œâ”€â”€ Limited compute: Can't run teacher inference
â”œâ”€â”€ Limited storage: Can't store teacher
â””â”€â”€ Limited power: Battery considerations
```

### Solutions
1. **Cached teacher outputs**: Pre-compute and store
2. **Partial teacher**: Load only necessary layers
3. **Progressive**: Distill incrementally
4. **Federated**: Distill across devices

### On-device Workflow
```
Cloud: Teacher inference â†’ Cache outputs
Edge:  Load cached outputs â†’ Train student locally
```

### Äá»c thÃªm
- On-device Training Survey (2021)
- Federated Knowledge Distillation (2020)

---

## 44. Task-specific Distillation for TinyML

### MÃ´ táº£
Customize distillation cho specific TinyML tasks nhÆ° keyword spotting, wake word, gesture recognition.

### TinyML Task Examples
| Task | Input | Output | Typical Size |
|------|-------|--------|--------------|
| Keyword spotting | Audio | Class | <50KB |
| Wake word | Audio | Binary | <20KB |
| Gesture | IMU data | Class | <30KB |
| Anomaly detection | Sensor | Binary | <10KB |

### Task-specific Considerations
```
Keyword Spotting:
â”œâ”€â”€ Temporal features important
â”œâ”€â”€ Spectral features secondary
â”œâ”€â”€ Distill along time axis
â””â”€â”€ Focus on phoneme representations
```

### Äá»c thÃªm
- TinyML KD (2021)
- Efficient Audio Classification (2020)

---

## 45. Progressive Distillation with Hardware Constraints

### MÃ´ táº£
Distill theo stages, dáº§n dáº§n reduce model size while respecting hardware limits at each stage.

### Progressive Schedule
```
Stage 1: Teacher â†’ Student-Large (2x compression)
Stage 2: Student-Large â†’ Student-Medium (4x total)
Stage 3: Student-Medium â†’ Student-Small (8x total)
Stage 4: Student-Small â†’ Student-Tiny (16x total)
```

### Benefits of Progressive Approach
- Easier optimization (smaller gap each step)
- Can stop at any stage
- Better final accuracy than direct distillation

### Hardware-aware Staging
```
At each stage, validate:
â”œâ”€â”€ Accuracy acceptable?
â”œâ”€â”€ Latency within budget?
â”œâ”€â”€ Memory fits device?
â””â”€â”€ Energy consumption OK?

If all yes â†’ continue to next stage
If accuracy < threshold â†’ stop
```

### Äá»c thÃªm
- Progressive Knowledge Distillation (2019)
- Staged Training (2020)

---

## ðŸ“š Distillation Toolbox

### Key Papers
1. "Distilling Knowledge in Neural Networks" (Hinton, 2015)
2. "FitNets" (2015)
3. "Knowledge Distillation Survey" (2020)

### Temperature Scaling
```python
# Soft targets with temperature
soft_targets = softmax(teacher_logits / temperature)
soft_predictions = softmax(student_logits / temperature)

KD_loss = KL_divergence(soft_predictions, soft_targets) * (temperature ** 2)
```

### Best Practices
- Higher temperature (T=4-20) for more knowledge transfer
- Combine with hard labels (Î± * hard + (1-Î±) * soft)
- Match intermediate representations
- Consider layer-wise learning rates
