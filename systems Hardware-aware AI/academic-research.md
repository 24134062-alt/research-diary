# NghiÃªn Cá»©u Há»c Thuáº­t: Hardware-aware AI

> **NgÃ y cáº­p nháº­t**: 2025-12-18  
> **Pháº¡m vi**: Tá»•ng há»£p cÃ¡c bÃ i bÃ¡o vÃ  nghiÃªn cá»©u há»c thuáº­t 2023-2024

---

## ğŸ“‘ Má»¥c Lá»¥c

1. [Survey Papers quan trá»ng](#1-survey-papers-quan-trá»ng)
2. [Neural Architecture Search (NAS)](#2-neural-architecture-search-nas)
3. [Quantization-aware Training](#3-quantization-aware-training)
4. [TinyML & Edge AI](#4-tinyml--edge-ai)
5. [Hardware Accelerators](#5-hardware-accelerators)
6. [CÃ¡c hÆ°á»›ng nghiÃªn cá»©u má»›i](#6-cÃ¡c-hÆ°á»›ng-nghiÃªn-cá»©u-má»›i)

---

## 1. Survey Papers Quan Trá»ng

### 1.1 Hardware-Aware Neural Architecture Search: Survey and Taxonomy
- **Venue**: IJCAI (International Joint Conference on Artificial Intelligence)
- **Ná»™i dung chÃ­nh**:
  - PhÃ¢n loáº¡i taxonomy cá»§a HW-NAS
  - ÄÃ¡nh giÃ¡ cÃ¡c chiáº¿n lÆ°á»£c Æ°á»›c tÃ­nh chi phÃ­ pháº§n cá»©ng
  - XÃ¢y dá»±ng mÃ´ hÃ¬nh DL hiá»‡u quáº£ Ä‘Ã¡p á»©ng rÃ ng buá»™c latency vÃ  nÄƒng lÆ°á»£ng
  - Trade-off giá»¯a accuracy vÃ  deployability trÃªn cÃ¡c platform khÃ¡c nhau

### 1.2 A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms
- **Timeline**: Submitted 06/2023, Major Revision 07/2024, Accepted for 03/2025
- **Ná»™i dung chÃ­nh**:
  - Tá»•ng há»£p cÃ¡c accelerator cho HPC applications
  - CÃ¡c loáº¡i accelerator: GPU-based, TPU, FPGA-based, ASIC-based
  - Neural Processing Units vÃ  co-processors trÃªn RISC-V
  - CÃ´ng nghá»‡ má»›i: 3D-stacked Processor-In-Memory, RRAM, PCM, Neuromorphic

### 1.3 Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI Models
- **Source**: arXiv, 03/2025
- **Ná»™i dung chÃ­nh**:
  - TÃ¬nh tráº¡ng hiá»‡n táº¡i, thÃ¡ch thá»©c vÃ  xu hÆ°á»›ng cá»§a on-device AI
  - NAS research táº­p trung vÃ o hardware-aware optimization
  - Äiá»u chá»‰nh neural architectures theo rÃ ng buá»™c cá»§a edge devices

---

## 2. Neural Architecture Search (NAS)

### 2.1 NASH: Neural Architecture Search for Hardware-Optimized ML Models
- **Source**: arXiv, 03/2024
- **ÄÃ³ng gÃ³p**:
  - Hardware-based architecture search algorithm
  - Sá»­ dá»¥ng FINN open-source hardware compiler
  - Tá»‘i Æ°u cho FPGA vá»›i quantized neural networks
  - TÃ­ch há»£p quantization vÃ o training algorithm
  - Cáº£i thiá»‡n accuracy vÃ  hardware resource utilization cho ResNet

### 2.2 QA-BWNAS: Scaling Up Quantization-Aware NAS for Efficient Deep Learning on Edge
- **Source**: arXiv, 01/2024
- **ÄÃ³ng gÃ³p**:
  - Quantization-aware NAS cho large-scale tasks
  - Block-wise formulation Ä‘á»ƒ tÃ¬m kiáº¿m architectures tá»‘i Æ°u
  - Káº¿t quáº£ cho semantic segmentation trÃªn Cityscapes dataset
  - Models nhá» hÆ¡n vÃ  nhanh hÆ¡n, khÃ´ng giáº£m performance

### 2.3 QuantNAS: Quantization-aware NAS for Efficient Mobile Deployment
- **Venue**: CVPR 2024 Workshops
- **ÄÃ³ng gÃ³p**:
  - Two-stage one-shot approach
  - TÃ¬m kiáº¿m architecture tá»« fully quantized supernet
  - Cáº£i thiá»‡n accuracy vÃ  giáº£m latency trÃªn mobile CPUs

### 2.4 JAQ: Joint Efficient Architecture Design and Low-Bit Quantization
- **Source**: Tsinghua University
- **ÄÃ³ng gÃ³p**:
  - JAQ Framework - Ä‘á»“ng tá»‘i Æ°u neural network architectures, quantization precisions, vÃ  hardware accelerators
  - Giáº£i quyáº¿t memory overhead vÃ  time-consuming hardware search
  - Higher accuracy vÃ  reduced hardware search time

### 2.5 An Affordable Hardware-Aware NAS for Ultra-Low-Power Computing Platforms
- **Timeline**: Expected 05/2024
- **ÄÃ³ng gÃ³p**:
  - HW-NAS cho ultra-low-power microcontrollers
  - Táº¡o tiny CNNs vá»›i state-of-the-art classification accuracy
  - Quy trÃ¬nh tÃ¬m kiáº¿m nháº¹ cÃ³ thá»ƒ cháº¡y trÃªn embedded devices

---

## 3. Quantization-aware Training

### 3.1 On-Chip Hardware-Aware Quantization (OHQ)
- **Source**: arXiv, 09/2023
- **ÄÃ³ng gÃ³p**:
  - Mixed-precision quantization trá»±c tiáº¿p trÃªn deployed edge devices
  - Nháº­n biáº¿t actual hardware efficiency
  - Æ¯á»›c tÃ­nh accuracy impact on-chip
  - Tá»‘i Æ°u bit-width configurations

### 3.2 Hardware-Aware Automated Quantization (HAQ) Framework
- **Timeline**: Prospective publication 06/2025
- **ÄÃ³ng gÃ³p**:
  - Reinforcement learning Ä‘á»ƒ tá»± Ä‘á»™ng xÃ¡c Ä‘á»‹nh quantization policies
  - TÃ­ch há»£p direct hardware feedback (latency vÃ  energy)
  - Hardware simulator trong design loop

### 3.3 HW-NAS for Quantized Neural Networks on FPGAs
- **Source**: Polytechnique MontrÃ©al Thesis, 12/2023
- **ÄÃ³ng gÃ³p**:
  - Sá»­a Ä‘á»•i DARTS framework cho gradient-based NAS
  - TÃ¬m kiáº¿m accurate vÃ  low-latency QNNs
  - FPGA implementation sá»­ dá»¥ng FINN environment
  - TÃ­ch há»£p latency nhÆ° optimization criterion
  - Giáº£m latency Ä‘Ã¡ng ká»ƒ vá»›i minimal accuracy drops

---

## 4. TinyML & Edge AI

### 4.1 Ká»¹ Thuáº­t Tá»‘i Æ¯u Model

| Ká»¹ thuáº­t | MÃ´ táº£ | Má»©c Ä‘á»™ nÃ©n |
|----------|-------|------------|
| **Pruning** | Loáº¡i bá» connections/neurons Ã­t quan trá»ng | 2-10x |
| **Quantization** | FP32 â†’ INT8/INT4 | 2-8x memory |
| **Knowledge Distillation** | Student model há»c tá»« teacher | Variable |
| **NAS** | Tá»± Ä‘á»™ng thiáº¿t káº¿ lightweight models | Task-specific |
| **Operator Fusion** | Gá»™p nhiá»u phÃ©p tÃ­nh thÃ nh kernel Ä‘Æ¡n | Reduce latency |

### 4.2 Hardware Platforms cho TinyML

| Platform | Äáº·c Ä‘iá»ƒm | Use Cases |
|----------|----------|-----------|
| **ARM Cortex-M** | Low-power MCUs, KB RAM | Wearables, sensors |
| **ESP32-S3** | WiFi/BLE, AI acceleration | IoT applications |
| **Arduino** | Beginner-friendly | Prototyping |
| **STM32** | Industrial-grade | Edge computing |
| **GAP-8** | Parallel processing, CNN accelerator | Vision AI |

### 4.3 Frameworks & Tools

| Tool | Chá»©c nÄƒng |
|------|-----------|
| **TensorFlow Lite Micro** | Lightweight TF cho embedded |
| **Edge Impulse** | End-to-end TinyML platform |
| **CMSIS-NN** | ARM optimized NN functions |
| **MCUNet** | Pre-trained super-networks |
| **Apache TVM** | Cross-platform compiler |
| **STM32Cube.AI** | STM32 optimization |

### 4.4 RÃ ng Buá»™c TÃ i NguyÃªn TinyML

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TINYML CONSTRAINTS                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Memory:        < 1MB (typically 64KB - 256KB)           â”‚
â”‚  Clock Speed:   40 - 400 MHz                             â”‚
â”‚  Power:         Milliwatts or less                       â”‚
â”‚  Model Size:    < 100KB (target)                         â”‚
â”‚  Latency:       Real-time (< 100ms)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Hardware Accelerators

### 5.1 PhÃ¢n Loáº¡i Accelerators

```
Hardware Accelerators
â”œâ”€â”€ GPU-based
â”‚   â”œâ”€â”€ NVIDIA CUDA cores
â”‚   â””â”€â”€ Tensor Cores (mixed precision)
â”œâ”€â”€ TPU (Tensor Processing Unit)
â”‚   â”œâ”€â”€ Google Cloud TPU
â”‚   â””â”€â”€ Edge TPU (Coral)
â”œâ”€â”€ FPGA-based
â”‚   â”œâ”€â”€ Xilinx (AMD)
â”‚   â””â”€â”€ Intel (Altera)
â”œâ”€â”€ ASIC-based
â”‚   â”œâ”€â”€ Neural Processing Units (NPU)
â”‚   â””â”€â”€ Custom accelerators
â””â”€â”€ Emerging Technologies
    â”œâ”€â”€ 3D-stacked Processor-In-Memory
    â”œâ”€â”€ Non-volatile Memory (RRAM, PCM)
    â”œâ”€â”€ Neuromorphic Processors
    â””â”€â”€ Multi-Chip Modules
```

### 5.2 Compute-in-Memory (CIM)

- **Váº¥n Ä‘á»**: "Memory wall" - bottleneck giá»¯a compute vÃ  memory
- **Giáº£i phÃ¡p**: TÃ­ch há»£p computing trá»±c tiáº¿p trong memory systems
- **Lá»£i Ã­ch**: 
  - Giáº£m data movement
  - TÄƒng energy efficiency
  - PhÃ¹ há»£p cho neural network operations (matrix multiplications)

### 5.3 Neuromorphic Computing

- **Ã tÆ°á»Ÿng**: Hardware mÃ´ phá»ng cáº¥u trÃºc vÃ  hoáº¡t Ä‘á»™ng cá»§a nÃ£o bá»™
- **Äáº·c Ä‘iá»ƒm**:
  - Event-driven processing
  - Spiking neural networks (SNNs)
  - Ultra-low power consumption
- **VÃ­ dá»¥**: Intel Loihi, IBM TrueNorth

---

## 6. CÃ¡c HÆ°á»›ng NghiÃªn Cá»©u Má»›i

### 6.1 Hardware-Software Co-design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CO-DESIGN PARADIGM                        â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚   Software  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    Hardware     â”‚               â”‚
â”‚   â”‚   (Models)  â”‚         â”‚ (Accelerators)  â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚          â”‚                         â”‚                         â”‚
â”‚          â–¼                         â–¼                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚           Compiler/Runtime               â”‚              â”‚
â”‚   â”‚    (Hardware-aware optimizations)        â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                         â”‚                                    â”‚
â”‚                         â–¼                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚      Performance/Energy/Accuracy         â”‚              â”‚
â”‚   â”‚           Feedback Loop                  â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Federated Learning on Edge

- **Má»¥c tiÃªu**: Private on-device training
- **ThÃ¡ch thá»©c**: 
  - Limited compute resources
  - Communication efficiency
  - Data heterogeneity
- **Giáº£i phÃ¡p**: Hardware-aware federated optimization

### 6.3 Emerging Research Topics

| Topic | MÃ´ táº£ | Potential Impact |
|-------|-------|------------------|
| **Tiny Deep Learning (TinyDL)** | Compressed DL models cho resource-constrained hardware | Democratize AI |
| **Photonic AI** | Light-based computing | Ultra-high speed, low energy |
| **DNA-based Computing** | Molecular computing | Massive parallelism |
| **Quantum ML** | Quantum speedup cho ML | Exponential speedup (theoretical) |

---

## ğŸ“Š Thá»‘ng KÃª Research Trends

### Papers theo nÄƒm (Æ°á»›c tÃ­nh)

| NÄƒm | HW-NAS | Quantization | TinyML | Total |
|-----|--------|--------------|--------|-------|
| 2021 | ~50 | ~80 | ~30 | ~160 |
| 2022 | ~80 | ~120 | ~60 | ~260 |
| 2023 | ~120 | ~180 | ~100 | ~400 |
| 2024 | ~180 | ~250 | ~150 | ~580 |

### Top Venues

- **Conferences**: NeurIPS, ICML, CVPR, ICCV, ICLR, DAC, HPCA
- **Journals**: TPAMI, TCAD, JSSC, Nature Electronics
- **Workshops**: MLSys, TinyML Summit, EfficientDL

---

## ğŸ”— Danh SÃ¡ch Papers Tham Kháº£o

### Survey Papers
1. "Hardware-Aware Neural Architecture Search: Survey and Taxonomy" - IJCAI
2. "A Survey on Deep Learning Hardware Accelerators for HPC Platforms" - arXiv 2024
3. "Empowering Edge Intelligence: Survey on On-Device AI Models" - arXiv 2025

### NAS Papers
4. "NASH: Neural Architecture Search for Hardware-Optimized ML Models" - arXiv 2024
5. "QA-BWNAS: Scaling Up Quantization-Aware NAS" - arXiv 2024
6. "QuantNAS: Quantization-aware NAS for Mobile" - CVPR 2024 Workshops
7. "JAQ: Joint Architecture and Quantization" - Tsinghua University

### Quantization Papers
8. "On-Chip Hardware-Aware Quantization (OHQ)" - arXiv 2023
9. "HAQ: Hardware-Aware Automated Quantization" - ResearchGate

### TinyML Papers
10. "TinyML: Machine Learning with TensorFlow Lite" - O'Reilly
11. "MCUNet: Tiny Deep Learning on IoT Devices" - NeurIPS 2020
12. "Once-for-All: Train One Network for All" - ICLR 2020

---

## ğŸ“ Ghi ChÃº NghiÃªn Cá»©u

### CÃ¢u há»i nghiÃªn cá»©u má»Ÿ

1. LÃ m tháº¿ nÃ o Ä‘á»ƒ tá»± Ä‘á»™ng hÃ³a hoÃ n toÃ n quÃ¡ trÃ¬nh co-design?
2. Trade-off tá»‘i Æ°u giá»¯a accuracy vÃ  efficiency lÃ  gÃ¬?
3. Generalization cá»§a HW-aware models qua cÃ¡c platforms?
4. CÃ¡ch tÃ­ch há»£p emerging hardware (neuromorphic, CIM) vÃ o ML frameworks?

### Potential research directions

- [ ] Hardware-aware training from scratch
- [ ] Cross-platform model optimization
- [ ] Energy-aware neural architecture search
- [ ] On-device learning with privacy guarantees

---

*Cáº­p nháº­t láº§n cuá»‘i: 2025-12-18*
