# ğŸ“š Learning Roadmap - Hardware-aware AI

> **Personalized cho báº¡n**  
> **Cáº­p nháº­t**: 12/2024

---

## âœ… Kiáº¿n Thá»©c ÄÃ£ CÃ³

| MÃ´n | Status | á»¨ng dá»¥ng |
|-----|--------|----------|
| Ma tráº­n (Linear Algebra) | âœ… ÄÃ£ há»c | Neural networks, quantization |
| Giáº£i tÃ­ch, Vi tÃ­ch phÃ¢n | âœ… ÄÃ£ há»c | Backpropagation, optimization |
| XÃ¡c suáº¥t thá»‘ng kÃª | âœ… ÄÃ£ há»c | ML foundations |
| C++ | âœ… ÄÃ£ há»c | Embedded, hardware interface |
| Python | âœ… ÄÃ£ há»c | ML frameworks |
| Cáº¥u trÃºc dá»¯ liá»‡u | âœ… ÄÃ£ há»c | Algorithms |

**â†’ Ná»n táº£ng tá»‘t! CÃ³ thá»ƒ báº¯t Ä‘áº§u ngay vá»›i ML.**

---

## ğŸ¯ HÆ°á»›ng Äi PhÃ¹ Há»£p Nháº¥t

| HÆ°á»›ng | Äá»™ phÃ¹ há»£p | Cáº§n há»c thÃªm |
|-------|-----------|--------------|
| **TinyML/Edge AI** | â­â­â­â­â­ | ML + Embedded |
| **Model Compression** | â­â­â­â­â­ | ML only |
| **Neuromorphic** | â­â­â­â­ | + Neuroscience basics |
| **Bio-inspired** | â­â­â­â­ | + Neuroscience |
| **CIM/ReRAM** | â­â­â­ | + Electronics/Circuits |
| **Quantum ML** | â­â­â­ | + Quantum Mechanics |
| **Photonic** | â­â­ | + Physics (Optics) |

---

## ğŸ—ºï¸ Lá»™ TrÃ¬nh 6 ThÃ¡ng

### Phase 1: Machine Learning (ThÃ¡ng 1-2)

```
Tuáº§n 1-2: PyTorch Fundamentals
â”œâ”€â”€ Tensors, autograd, nn.Module
â”œâ”€â”€ Dataset, DataLoader
â””â”€â”€ ğŸ”— pytorch.org/tutorials

Tuáº§n 3-4: Neural Networks
â”œâ”€â”€ MLP â†’ MNIST classification
â”œâ”€â”€ CNN â†’ CIFAR-10 classification
â”œâ”€â”€ Loss functions, optimizers
â””â”€â”€ ğŸ”— Course: Fast.ai (practical)

Tuáº§n 5-6: Training Techniques
â”œâ”€â”€ Regularization (dropout, batch norm)
â”œâ”€â”€ Learning rate scheduling
â”œâ”€â”€ Data augmentation
â””â”€â”€ ğŸ¯ Goal: Train ResNet-18 on CIFAR-10

Tuáº§n 7-8: Modern Architectures
â”œâ”€â”€ Transformers, Attention mechanism
â”œâ”€â”€ Vision Transformer (ViT) basics
â””â”€â”€ ğŸ”— Course: Karpathy "Let's build GPT"
```

**ğŸ“š Resources:**
- [Fast.ai](https://course.fast.ai/) - FREE, practical
- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [Andrej Karpathy YouTube](https://www.youtube.com/@AndrejKarpathy)

---

### Phase 2: Hardware & Optimization (ThÃ¡ng 3-4)

```
Tuáº§n 9-10: Computer Architecture
â”œâ”€â”€ Memory hierarchy (Cache, DRAM)
â”œâ”€â”€ Latency vs bandwidth
â”œâ”€â”€ GPU architecture basics
â””â”€â”€ ğŸ”— Book: Patterson & Hennessy Ch.1-5

Tuáº§n 11-12: Model Compression Basics
â”œâ”€â”€ Quantization (FP32 â†’ INT8)
â”œâ”€â”€ Pruning (structured, unstructured)
â”œâ”€â”€ Knowledge distillation
â””â”€â”€ ğŸ”— PyTorch quantization tutorial

Tuáº§n 13-14: Hands-on Compression
â”œâ”€â”€ Quantize your ResNet-18
â”œâ”€â”€ Prune 50% weights
â”œâ”€â”€ Measure speedup
â””â”€â”€ ğŸ¯ Goal: 2-4x smaller model, <2% accuracy loss

Tuáº§n 15-16: Profiling & Analysis
â”œâ”€â”€ torch.profiler
â”œâ”€â”€ Memory usage analysis
â”œâ”€â”€ Latency breakdown
â””â”€â”€ Compare FP32 vs INT8 performance
```

**ğŸ“š Resources:**
- [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)
- [Neural Network Distiller](https://nervanasystems.github.io/distiller/)
- Book: "Computer Organization and Design" (Patterson)

---

### Phase 3: Specialization (ThÃ¡ng 5-6)

**Chá»n 1 trong 2 track:**

#### Track A: TinyML/Edge AI ğŸ“±

```
Tuáº§n 17-18: TensorFlow Lite
â”œâ”€â”€ Convert PyTorch â†’ TFLite
â”œâ”€â”€ INT8 quantization for mobile
â””â”€â”€ ğŸ”— tensorflow.org/lite

Tuáº§n 19-20: Edge Deployment
â”œâ”€â”€ Deploy on Raspberry Pi
â”œâ”€â”€ hoáº·c ESP32/Arduino
â”œâ”€â”€ Real-time inference
â””â”€â”€ ğŸ”— Edge Impulse tutorials

Tuáº§n 21-22: Optimization
â”œâ”€â”€ Profile on device
â”œâ”€â”€ Memory optimization
â”œâ”€â”€ Latency optimization
â””â”€â”€ ğŸ¯ Goal: <100ms inference on edge

Tuáº§n 23-24: Project
â”œâ”€â”€ Build complete edge AI application
â”œâ”€â”€ Object detection hoáº·c voice recognition
â””â”€â”€ Document & share on GitHub
```

#### Track B: Neuromorphic Computing ğŸ§ 

```
Tuáº§n 17-18: Spiking Neural Networks
â”œâ”€â”€ LIF neuron model
â”œâ”€â”€ snnTorch framework
â””â”€â”€ ğŸ”— snntorch.readthedocs.io

Tuáº§n 19-20: SNN Training
â”œâ”€â”€ Surrogate gradient method
â”œâ”€â”€ Train SNN on MNIST
â”œâ”€â”€ Compare with ANN
â””â”€â”€ ğŸ”— Tutorials on snnTorch

Tuáº§n 21-22: Neuromorphic Concepts
â”œâ”€â”€ Event-driven processing
â”œâ”€â”€ STDP learning rule
â”œâ”€â”€ Neuromorphic datasets (N-MNIST)
â””â”€â”€ ğŸ”— Intel Lava framework

Tuáº§n 23-24: Project
â”œâ”€â”€ SNN for gesture recognition / keyword spotting
â”œâ”€â”€ Compare energy vs ANN
â””â”€â”€ Document & share
```

---

## ğŸ“… Weekly Schedule Template

```
Má»—i tuáº§n (10-15 giá»):

Thá»© 2-3: Theory (2-3h)
â”œâ”€â”€ Äá»c papers/tutorials
â””â”€â”€ Watch lectures

Thá»© 4-5: Coding (4-5h)
â”œâ”€â”€ Implement concepts
â””â”€â”€ Run experiments

Thá»© 6-7: Project (3-4h)
â”œâ”€â”€ Apply to personal project
â””â”€â”€ Debug, iterate

Chá»§ nháº­t: Review (1-2h)
â”œâ”€â”€ Summarize learnings
â””â”€â”€ Plan next week
```

---

## ğŸ› ï¸ Tools Setup

### Environment Setup (Do This First!)

```bash
# 1. Install Miniconda
# Download from: https://docs.conda.io/en/latest/miniconda.html

# 2. Create environment
conda create -n hwai python=3.10
conda activate hwai

# 3. Install PyTorch
pip install torch torchvision torchaudio

# 4. Install tools
pip install numpy matplotlib jupyter
pip install tensorboard wandb  # logging

# 5. (Optional) TinyML tools
pip install tensorflow tflite-runtime

# 6. (Optional) Neuromorphic
pip install snntorch
```

---

## ğŸ“Š Progress Tracker

### Phase 1: Machine Learning
- [ ] PyTorch basics completed
- [ ] MLP on MNIST (>98% acc)
- [ ] CNN on CIFAR-10 (>85% acc)
- [ ] Understand Transformers
- [ ] ResNet-18 trained from scratch

### Phase 2: Hardware & Optimization
- [ ] Understand memory hierarchy
- [ ] INT8 quantization implemented
- [ ] Pruning implemented
- [ ] Model size reduced 2-4x
- [ ] Profiling completed

### Phase 3: Specialization
- [ ] Track chosen: ____________
- [ ] Framework learned
- [ ] First project completed
- [ ] GitHub repo published
- [ ] (Optional) Blog post written

---

## ğŸ“š Essential Resources

### Courses (Free)

| Course | Platform | Focus |
|--------|----------|-------|
| Fast.ai | fast.ai | Practical DL |
| CS231n | YouTube | CNNs |
| Let's build GPT | YouTube | Transformers |
| TinyML | edX | Edge AI |

### Books

| Book | Focus | Priority |
|------|-------|----------|
| "Deep Learning" (Goodfellow) | Theory | Medium |
| "Dive into DL" (d2l.ai) | Practical | High |
| "TinyML" (O'Reilly) | Edge AI | High (if Track A) |

### Communities

- **Discord**: ML Collective
- **Reddit**: r/MachineLearning, r/learnmachinelearning
- **Twitter/X**: Follow researchers

---

## ğŸ¯ 6-Month Milestones

| Month | Milestone | Deliverable |
|-------|-----------|-------------|
| 1 | PyTorch proficiency | CIFAR-10 CNN |
| 2 | Modern architectures | Transformer implementation |
| 3 | Compression basics | Quantized model |
| 4 | Optimization skills | 4x compressed model |
| 5 | Specialization | Track-specific project |
| 6 | Complete project | GitHub + Documentation |

---

**Báº¯t Ä‘áº§u ngay vá»›i Phase 1, Tuáº§n 1: PyTorch Fundamentals! ğŸš€**

*Tip: Äáº·t reminder há»c má»—i ngÃ y, consistency > intensity*
